{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11976467,"sourceType":"datasetVersion","datasetId":7531680},{"sourceId":11987237,"sourceType":"datasetVersion","datasetId":7539571},{"sourceId":11988505,"sourceType":"datasetVersion","datasetId":7540437}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup and Initial Configuration","metadata":{}},{"cell_type":"code","source":"# --- Essential Installs ---\n!pip install -q transformers datasets accelerate accelerate \n!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n!pip install -q pytorch-lightning\n!pip install -q nltk spacy scikit-learn torchmetrics pandas summa sentence_transformers\n!pip install -q git+https://github.com/google-research/bleurt.git \n!pip install -q git+https://github.com/neulab/BARTScore.git \n!pip install -q ctc-score bert-score \n\n# Check transformers version\n!pip show transformers\n\n# --- Clone BARTScore Repository ---\nimport os\nKAGGLE_WORKING_DIR = \"/kaggle/working/\" \nBARTSCORE_DIR = os.path.join(KAGGLE_WORKING_DIR, \"BARTScore_repo\")\n\nif not os.path.exists(BARTSCORE_DIR):\n    print(\"Cloning BARTScore repository...\")\n    !git clone https://github.com/neulab/BARTScore.git {BARTSCORE_DIR}\n    print(\"BARTScore cloned.\")\nelse:\n    print(\"BARTScore repository already cloned.\")\n\n# Add BARTScore to Python Path\nimport sys\nif BARTSCORE_DIR not in sys.path:\n    sys.path.append(BARTSCORE_DIR)\n    print(f\"Added {BARTSCORE_DIR} to sys.path\")\n\n# --- Imports ---\nimport json\nimport random\nimport math\nimport re\nimport pickle\nimport logging\nimport time\nfrom argparse import Namespace\n\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader, Dataset, Sampler\nfrom tqdm.auto import tqdm # Good practice for notebooks\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, precision_recall_fscore_support,\n    roc_auc_score, matthews_corrcoef, balanced_accuracy_score\n)\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.stats import pearsonr, kendalltau, spearmanr # Added spearmanr based on evaluate.py\nfrom nltk.tokenize import sent_tokenize\nimport nltk\nimport spacy\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import LightningDataModule\n\nfrom datasets import load_dataset \nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer,\n    AutoModel,\n    AutoModelForSequenceClassification,\n    AutoModelForMaskedLM, \n    BertForPreTraining, BertModel, RobertaModel, AlbertModel, AlbertForMaskedLM, RobertaForMaskedLM,\n    T5Tokenizer, T5ForConditionalGeneration,\n    BartTokenizer, BartForConditionalGeneration\n)\n\n# Attempt to import AdamW and get_linear_schedule_with_warmup\ntry:\n    from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n    print(\"Successfully imported AdamW from transformers.optimization\")\nexcept ImportError:\n    print(\"Could not import AdamW from transformers.optimization. Attempting from torch.optim...\")\n    try:\n        from torch.optim import AdamW\n        from transformers import get_linear_schedule_with_warmup \n        print(\"Successfully imported AdamW from torch.optim and get_linear_schedule_with_warmup from transformers\")\n    except ImportError:\n        print(\"Error: AdamW not found in transformers.optimization or torch.optim. Scheduler setup might fail.\")\n\n\n# --- Basic Configuration ---\n# KAGGLE_WORKING_DIR already defined\nKAGGLE_INPUT_DIR = \"/kaggle/input/\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n# KAGGLE_INPUT_DIR = \"/kaggle/input/samples-fact-checking-adjacent-datasets\"\n\nSAMPLES_PER_DATASET_TARGET = 20000 \nGENERATED_DATA_DIR = os.path.join(KAGGLE_WORKING_DIR, \"alignscore_training_data_20k\") \nCHECKPOINT_SAVE_PATH = os.path.join(KAGGLE_WORKING_DIR, \"alignscore_checkpoints\") \nMODEL_NAME = \"roberta-large\" \n\nos.makedirs(GENERATED_DATA_DIR, exist_ok=True)\nos.makedirs(CHECKPOINT_SAVE_PATH, exist_ok=True)\n\nnltk.download('punkt', quiet=True)\nnltk.download('stopwords', quiet=True)\ntry:\n    spacy.load('en_core_web_sm')\nexcept OSError:\n    print(\"Downloading spacy en_core_web_sm model...\")\n    spacy.cli.download('en_core_web_sm')\n    print(\"Spacy model downloaded.\")\n\n\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\nlogging.getLogger(\"datasets\").setLevel(logging.ERROR)\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.INFO)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-28T23:04:32.536338Z","iopub.execute_input":"2025-05-28T23:04:32.536587Z","iopub.status.idle":"2025-05-28T23:07:42.005660Z","shell.execute_reply.started":"2025-05-28T23:04:32.536571Z","shell.execute_reply":"2025-05-28T23:07:42.004931Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for BLEURT (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: git+https://github.com/neulab/BARTScore.git does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hName: transformers\nVersion: 4.51.3\nSummary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\nHome-page: https://github.com/huggingface/transformers\nAuthor: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\nAuthor-email: transformers@huggingface.co\nLicense: Apache 2.0 License\nLocation: /usr/local/lib/python3.11/dist-packages\nRequires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\nRequired-by: bert-score, ctc-score, kaggle-environments, peft, sentence-transformers\nCloning BARTScore repository...\nCloning into '/kaggle/working/BARTScore_repo'...\nremote: Enumerating objects: 220, done.\u001b[K\nremote: Counting objects: 100% (26/26), done.\u001b[K\nremote: Compressing objects: 100% (12/12), done.\u001b[K\nremote: Total 220 (delta 18), reused 14 (delta 14), pack-reused 194 (from 1)\u001b[K\nReceiving objects: 100% (220/220), 101.98 MiB | 22.69 MiB/s, done.\nResolving deltas: 100% (47/47), done.\nUpdating files: 100% (192/192), done.\nBARTScore cloned.\nAdded /kaggle/working/BARTScore_repo to sys.path\n","output_type":"stream"},{"name":"stderr","text":"2025-05-28 23:07:24.656759: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748473645.079217      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748473645.208509      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Could not import AdamW from transformers.optimization. Attempting from torch.optim...\nSuccessfully imported AdamW from torch.optim and get_linear_schedule_with_warmup from transformers\nUsing device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Data Generation Definitions","metadata":{}},{"cell_type":"code","source":"# Cell 2: Data Generation Definitions (Full, with Indentation Review)\n\nimport os\nimport json\nimport random\nimport math\nimport re\nimport pickle\nimport logging\nimport time\nfrom tqdm.auto import tqdm # Use auto version for better notebook compatibility\n\n# Ensure these are available if not imported in Cell 1, or if Cell 1 is not run before this\ntry:\n    from datasets import load_dataset\nexcept ImportError:\n    print(\"WARNING: `load_dataset` from `datasets` library not imported. This cell will fail.\")\n    # You might need !pip install datasets in Cell 1 if it's missing\n\ntry:\n    from transformers import AutoTokenizer, AutoModelForMaskedLM, BartTokenizer, BartForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\nexcept ImportError:\n    print(\"WARNING: Hugging Face Transformers not fully imported. This cell will fail.\")\n    # You might need !pip install transformers in Cell 1\n\ntry:\n    import torch # Ensure torch is imported for DEVICE\n    if 'DEVICE' not in globals(): # If DEVICE not defined in Cell 1 (e.g. running cells out of order)\n        DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Cell 2: DEVICE automatically set to {DEVICE}\")\nexcept ImportError:\n    print(\"WARNING: PyTorch not imported. This cell will fail.\")\n    DEVICE = \"cpu\" # Fallback, but GPU dependent classes will fail\n\ntry:\n    from summa.summarizer import summarize\nexcept ImportError:\n    print(\"WARNING: `summa` library not imported. ExtractiveSummarizationGenerator will fail.\")\n    # You might need !pip install summa in Cell 1\n\n\n# --- DATASET_HUGGINGFACE and DATASET_CONFIG ---\nDATASET_HUGGINGFACE = {\n    'cnndm': ['cnn_dailymail', '3.0.0', 'train'],\n    'mnli': ['multi_nli', 'default', 'train'],\n    'squad_v2': ['squad_v2', 'squad_v2', 'train'],\n    'paws': ['paws', 'labeled_final', 'train'], \n    'vitaminc': ['tals/vitaminc', 'default', 'train'],\n    'xsum': ['xsum', 'default', 'train'],\n    'stsb': ['glue', 'stsb', 'train'],\n    'sick': ['sick', 'default', 'train'],\n    'race': ['race', 'all', 'train'],\n    'anli_r1': ['anli', 'plain_text', 'train_r1'],\n    'anli_r2': ['anli', 'plain_text', 'train_r2'],\n    'anli_r3': ['anli', 'plain_text', 'train_r3'],\n    'snli': ['snli', 'plain_text', 'train'],\n    'wikihow': ['wikihow', 'all', 'train'],\n    'mrpc': ['glue', 'mrpc', 'train'],\n    'qqp': ['glue', 'qqp', 'train'],\n    'adversarial_qa': ['adversarial_qa', 'adversarialQA', 'train'],\n    'drop': ['drop', 'train'],\n    'duorc_self': ['duorc', 'SelfRC', 'train'],\n    'duorc_paraphrase': ['duorc', 'ParaphraseRC', 'train'],\n    'quoref': ['quoref', 'train'],\n    'hotpot_qa_distractor': ['hotpot_qa', 'distractor', 'train'],\n    'hotpot_qa_fullwiki': ['hotpot_qa', 'fullwiki', 'train'],\n    'ropes': ['ropes', 'train'],\n    'boolq': ['boolq', 'train'],\n    'eraser_multi_rc': ['eraser_multi_rc', 'train'],\n    'quail': ['quail', 'train'],\n    'sciq': ['sciq', 'train'],\n    'strategy_qa': ['metaeval/strategy-qa', 'train'],\n    'gap': ['gap', 'train'],\n    'doc_nli': [\"saattrupdan/doc-nli\", None, \"train\"],\n    'nli_fever': [\"pietrolesci/nli_fever\", \"default\", \"train\"],\n    'newsqa': [\"newsqa\", \"default\", \"train\"],\n}\n\nDATASET_CONFIG = {\n    'cnndm': {'task': 'summarization', 'text_a': 'article', 'text_b': 'highlights', 'label': None, 'huggingface': True},\n    'mnli': {'task': 'nli', 'text_a': 'premise', 'text_b': 'hypothesis', 'label': 'label', 'huggingface': True},\n    'squad_v2': {'task': 'qa', 'text_a': 'context', 'text_b': ['question', 'answers'], 'label': None, 'huggingface': True, 'method_name': 'squad_v2_new'},\n    'paws': {'task': 'paraphrase', 'text_a': 'sentence1', 'text_b': 'sentence2', 'label': 'label', 'huggingface': True},\n    'vitaminc': {'task': 'fact_checking', 'text_a': 'evidence', 'text_b': 'claim', 'label': 'label', 'huggingface': True},\n    'xsum': {'task': 'summarization', 'text_a': 'document', 'text_b': 'summary', 'label': None, 'huggingface': True},\n    'stsb': {'task': 'sts', 'text_a': 'sentence1', 'text_b': 'sentence2', 'label': 'label', 'huggingface': True},\n    'sick': {'task': 'sts', 'text_a': 'sentence_A', 'text_b': 'sentence_B', 'label': 'relatedness_score', 'huggingface': True},\n    'race': {'task': 'qa', 'text_a': 'article', 'text_b': ['question', 'options'], 'label': 'answer', 'huggingface': True},\n    'anli_r1': {'task': 'nli', 'text_a': 'premise', 'text_b': 'hypothesis', 'label': 'label', 'huggingface': True},\n    'anli_r2': {'task': 'nli', 'text_a': 'premise', 'text_b': 'hypothesis', 'label': 'label', 'huggingface': True},\n    'anli_r3': {'task': 'nli', 'text_a': 'premise', 'text_b': 'hypothesis', 'label': 'label', 'huggingface': True},\n    'snli': {'task': 'nli', 'text_a': 'premise', 'text_b': 'hypothesis', 'label': 'label', 'huggingface': True},\n    'wikihow': {'task': 'summarization', 'text_a': 'text', 'text_b': 'headline', 'label': None, 'huggingface': True},\n    'mrpc': {'task': 'paraphrase', 'text_a': 'sentence1', 'text_b': 'sentence2', 'label': 'label','huggingface': True},\n    'qqp': {'task': 'paraphrase', 'text_a':'question1', 'text_b':'question2', 'label': 'label', 'huggingface': True},\n    'adversarial_qa': {'task': 'qa', 'text_a': 'context', 'text_b': ['question', 'answers'],'label': None, 'huggingface': True},\n    'drop': {'task': 'qa', 'text_a': 'passage', 'text_b': ['question', 'answers_spans'], 'label': 'answers_spans', 'huggingface': True},\n    'duorc_self': {'task': 'qa', 'text_a': 'plot', 'text_b': ['question', 'answers'], 'label': 'answers', 'huggingface': True},\n    'duorc_paraphrase': {'task': 'qa', 'text_a': 'plot', 'text_b': ['question', 'answers'], 'label': 'answers', 'huggingface': True},\n    'quoref': {'task': 'qa', 'text_a': 'context', 'text_b': ['question', 'answers'], 'label': 'answers', 'huggingface': True},\n    'hotpot_qa_distractor': {'task': 'qa', 'huggingface': True},\n    'hotpot_qa_fullwiki': {'task': 'qa', 'huggingface': True},\n    'ropes': {'task': 'qa', 'text_a': ['background', 'situation'], 'text_b': ['question', 'answers'], 'label': 'answers', 'huggingface': True},\n    'boolq': {'task': 'qa', 'text_a': 'passage', 'text_b': 'question', 'label': 'answer', 'huggingface': True},\n    'eraser_multi_rc': {'task': 'qa', 'text_a':'passage', 'text_b':'query_and_answer', 'label':'label', 'huggingface': True},\n    'quail': {'task': 'qa', 'text_a':'context', 'text_b':['question','answers'], 'label':'correct_answer_id', 'huggingface': True},\n    'sciq': {'task': 'qa', 'text_a':'support', 'text_b':['question','correct_answer','distractor1','distractor2','distractor3'], 'label':'correct_answer', 'huggingface': True},\n    'strategy_qa': {'task': 'qa', 'text_a':'facts', 'text_b':'question', 'label':'answer', 'huggingface': True},\n    'gap': {'task': 'coreference', 'text_a':'Text', 'text_b':['Pronoun', 'A', 'B'], 'label':['A-coref', 'B-coref'], 'huggingface': True},\n    'doc_nli': {'task': 'bin_nli', 'text_a': 'premise', 'text_b': 'hypothesis', 'label': 'label', 'huggingface': True},\n    'nli_fever': {'task': 'nli', 'text_a': 'premise', 'text_b': 'hypothesis', 'label': 'label', 'huggingface': True },\n    'newsqa': {'task': 'qa', 'huggingface': True},\n}\n\n# --- Helper Classes ---\nclass QA2D:\n    def __init__(self, batch_size=32, device='cuda', verbose=True) -> None:\n        from transformers import BartTokenizer, BartForConditionalGeneration\n        self.tokenizer = BartTokenizer.from_pretrained(\"MarkS/bart-base-qa2d\")\n        self.model = BartForConditionalGeneration.from_pretrained(\"MarkS/bart-base-qa2d\").to(device)\n        self.batch_size = batch_size\n        self.device=device\n        self.verbose = verbose\n\n    def generate(self, questions: list, answers: list):\n        assert len(questions) == len(answers)\n        qa_list = []\n        for q, a in zip(questions, answers):\n            qa_list.append(f\"question: {q} answer: {a}\")\n        output = []\n        for qa_pairs in tqdm(\n            self.chunks(qa_list, self.batch_size),\n            desc=\"QA to Declarative\",\n            total=math.ceil(len(qa_list)/self.batch_size),\n            disable=(not self.verbose)\n        ):\n            input_text = qa_pairs\n            input_token = self.tokenizer(\n                input_text, return_tensors='pt', padding=True, truncation=True).to(self.device)\n            dec_sents = self.model.generate(\n                 input_token.input_ids, max_length=512)\n            result = self.tokenizer.batch_decode(\n                dec_sents, skip_special_tokens=True)\n            output.extend(result)\n        return output\n\n    def chunks(self, lst, n):\n        for i in range(0, len(lst), n):\n             yield lst[i:i + n]\n\nclass QAnswering:\n    def __init__(self, batch_size=32, device='cuda', verbose=True) -> None:\n        from transformers import T5Tokenizer, T5ForConditionalGeneration\n        self.tokenizer = T5Tokenizer.from_pretrained(\"valhalla/t5-base-qa-qg-hl\")\n        self.model = T5ForConditionalGeneration.from_pretrained(\"valhalla/t5-base-qa-qg-hl\").to(device)\n        self.batch_size = batch_size\n        self.device = device\n        self.verbose = verbose\n\n    def generate(self, questions: list, contexts: list):\n        assert len(questions) == len(contexts)\n        answers = []\n        for qs, cs in tqdm(zip(self.chunks(questions, self.batch_size), self.chunks(contexts, self.batch_size)), desc=\"Generating Answers for not answerable\", total=math.ceil(len(questions)/self.batch_size), disable=(not self.verbose)):\n            qc_pairs = []\n            assert len(qs) == len(cs)\n            for one_q, one_c in zip(qs, cs):\n                qc_pairs.append(f\"\"\"question: {one_q} context: {one_c}\"\"\")\n            input_ids = self.tokenizer(\n                qc_pairs, padding=True, truncation=True, return_tensors='pt').to(self.device).input_ids\n            outputs = self.model.generate(input_ids, max_length=512)\n            answers.extend(self.tokenizer.batch_decode(\n                 outputs, skip_special_tokens=True))\n        return answers\n\n    def chunks(self, lst, n):\n        for i in range(0, len(lst), n):\n            yield lst[i:i + n]\n\nclass MLMGeneratorWithPairedData:\n    def __init__(self, corpra: list, device='cuda', batch_size=8, mask_percent=0.25, verbose=True) -> None:\n        self.device = device\n        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n        self.model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\").to(self.device)\n        self.mask_percent = mask_percent\n        self.batch_size = batch_size\n        self.dataset = corpra \n        self.verbose = verbose\n        if self.verbose: print(f\"MLMGenerator: Initialized with {len(corpra)} sentences. Batch size: {batch_size}.\")\n\n    def chunks(self, lst, n):\n        for i in range(0, len(lst), n):\n             yield lst[i:i + n]\n\n    def generate(self):\n        sents_output = []\n        if not self.dataset:\n            if self.verbose: print(\"MLMGenerator.generate: Input corpus is empty. Returning empty list.\")\n            return sents_output\n\n        if self.verbose: print(f\"MLMGenerator.generate: Starting. Corpus size: {len(self.dataset)}, Batch size: {self.batch_size}\")\n        \n        chunk_iterator = self.chunks(self.dataset, self.batch_size)\n        num_chunks = math.ceil(len(self.dataset) / self.batch_size) if self.dataset else 0\n\n        if self.verbose and num_chunks == 0 and self.dataset : \n             print(\"MLMGenerator.generate: Warning - num_chunks is 0 but dataset is not empty. This is unexpected.\")\n\n        for i, examples_batch in tqdm(enumerate(chunk_iterator), total=num_chunks, desc=\"MLM Batch Processing\", disable=(not self.verbose)):\n            if self.verbose: print(f\"MLMGenerator.generate: Processing chunk {i+1}/{num_chunks}. Batch items: {len(examples_batch)}\")\n            \n            sents_to_be_noised = [str(each) for each in examples_batch] \n            if not sents_to_be_noised:\n                if self.verbose: print(f\"MLMGenerator.generate: Chunk {i+1} is empty. Skipping.\")\n                continue\n                \n            if self.verbose and sents_to_be_noised: print(f\"MLMGenerator.generate: First sentence in batch to be noised (chunk {i+1}): '{sents_to_be_noised[0][:100]}...'\")\n            \n            sents_noised = self.mlm_infiller(sents_to_be_noised) \n            \n            if self.verbose: print(f\"MLMGenerator.generate: Finished mlm_infiller for chunk {i+1}. Num noised sents from chunk: {len(sents_noised)}\")\n            sents_output.extend(sents_noised)\n        \n        if self.verbose: print(f\"MLMGenerator.generate: Finished generate() method. Total sents_output: {len(sents_output)}\")\n        return sents_output\n\n    def mlm_infiller(self, batch_of_sents):\n        if self.verbose: print(f\"  MLMGenerator.mlm_infiller: Starting for a batch of {len(batch_of_sents)} sentences.\")\n        \n        masked_batch_input_ids = []\n        for sent_idx, each_sent in enumerate(batch_of_sents):\n            if self.verbose and sent_idx < 2: \n                print(f\"    MLMGenerator.mlm_infiller: Processing sentence {sent_idx+1} in batch: '{each_sent[:100]}...'\")\n            \n            token_ids = self.tokenizer.encode(each_sent, add_special_tokens=False, truncation=True, max_length=self.tokenizer.model_max_length - 2)\n            \n            if not token_ids: \n                if self.verbose: print(f\"    MLMGenerator.mlm_infiller: Sentence {sent_idx+1} resulted in empty token_ids. Appending dummy CLS/SEP.\")\n                masked_batch_input_ids.append([self.tokenizer.cls_token_id, self.tokenizer.sep_token_id])\n                continue\n\n            num_to_mask = max(1, int(self.mask_percent * len(token_ids)))\n            if len(token_ids) < num_to_mask: # Should not happen if token_ids is not empty and num_to_mask is max 1\n                num_to_mask = len(token_ids)\n\n            mask_indices = sorted(random.sample(list(range(len(token_ids))), num_to_mask))\n            \n            current_masked_ids = list(token_ids)\n\n            for idx_in_token_ids in mask_indices:\n                prob = random.random()\n                if prob < 0.8: \n                    current_masked_ids[idx_in_token_ids] = self.tokenizer.mask_token_id\n                elif prob < 0.9: \n                    current_masked_ids[idx_in_token_ids] = random.randint(0, self.tokenizer.vocab_size - 1)\n\n            final_input_ids = [self.tokenizer.cls_token_id] + current_masked_ids + [self.tokenizer.sep_token_id]\n            masked_batch_input_ids.append(final_input_ids)\n        \n        if not masked_batch_input_ids:\n            if self.verbose: print(\"  MLMGenerator.mlm_infiller: No valid sequences to process after initial tokenization checks.\")\n            return [\"\"] * len(batch_of_sents) # Return expected number of empty strings\n\n        if self.verbose: print(f\"  MLMGenerator.mlm_infiller: Preparing to pad {len(masked_batch_input_ids)} sequences.\")\n        try:\n            inputs = self.tokenizer.pad(\n                {\"input_ids\": masked_batch_input_ids},\n                padding=True, \n                return_attention_mask=True, \n                return_tensors=\"pt\"\n            ).to(self.device)\n        except Exception as e:\n            if self.verbose: print(f\"  MLMGenerator.mlm_infiller: Error during tokenizer.pad: {e}\")\n            for idx, seq_ids in enumerate(masked_batch_input_ids):\n                 if self.verbose: print(f\"    Sequence {idx} length: {len(seq_ids)}, content: {seq_ids[:10]}...\")\n            return [\"\"] * len(batch_of_sents) \n\n        if self.verbose: print(f\"  MLMGenerator.mlm_infiller: Padded inputs created. Shape: {inputs['input_ids'].shape}. Calling model...\")\n        try:\n            with torch.no_grad():\n                logits = self.model(**inputs).logits\n        except Exception as e:\n            if self.verbose: print(f\"  MLMGenerator.mlm_infiller: Error during model inference: {e}\")\n            return [\"\"] * len(batch_of_sents) \n\n        if self.verbose: print(f\"  MLMGenerator.mlm_infiller: Model inference complete. Logits shape: {logits.shape}\")\n\n        infilled_sents = []\n        for i in range(len(masked_batch_input_ids)): \n            sent_logits = logits[i] \n            original_sequence_before_padding = masked_batch_input_ids[i] \n            reconstructed_ids = list(original_sequence_before_padding) \n\n            mask_positions_in_original = [idx for idx, token_id in enumerate(original_sequence_before_padding) if token_id == self.tokenizer.mask_token_id]\n\n            for pos_in_orig in mask_positions_in_original:\n                 predicted_token_id = sent_logits[pos_in_orig].argmax(axis=-1).item()\n                 reconstructed_ids[pos_in_orig] = predicted_token_id\n            \n            infilled_sent_text = self.tokenizer.decode(reconstructed_ids, skip_special_tokens=True)\n            infilled_sents.append(infilled_sent_text)\n\n        if self.verbose: print(f\"  MLMGenerator.mlm_infiller: Finished for batch. Returning {len(infilled_sents)} sentences.\")\n        return infilled_sents \n\nclass ExtractiveSummarizationGenerator:\n    def __init__(self) -> None:\n        pass\n\n    def generate(self, texts, verbose=True):\n        from summa.summarizer import summarize \n        summaries = []\n        for text in tqdm(texts, desc=\"Extracting Summary\", leave=False, disable=(not verbose)):\n            summ = \"\"\n            try:\n                num_words = len(text.split())\n                if num_words > 10: # Avoid summa errors on very short texts\n                    target_ratio = max(0.05, min(0.3, 50.0 / num_words if num_words > 50 else 0.2)) # Ensure ratio isn't too small\n                    summ = summarize(text, ratio=target_ratio)\n                if not summ.strip() and num_words > 20: \n                    summ = summarize(text, words=min(50, num_words // 2 if num_words // 2 > 10 else 10))\n            except Exception as e:\n                # if verbose: print(f\"Summa error for text '{text[:50]}...': {e}\")\n                pass \n            summaries.append(summ.strip() if summ else text.split('.')[0] if '.' in text else text[:100]) \n        return summaries\n\n# --- DataGenerator Class ---\nclass DataGenerator:\n    def __init__(self, dataset_names, target_samples_per_dataset=20000, verbose=True):\n        self.dataset_names = dataset_names\n        self.datasets = dict()\n        self.target_samples = target_samples_per_dataset\n        self.verbose = verbose\n        self.device = DEVICE\n\n        self._qa2d_generator = None\n        self._qa_generator = None\n        self._mlm_generators = {} \n        self._ext_summarizer = None\n\n        self.load_dataset_from_huggingface()\n\n    @property\n    def qa2d_generator(self):\n        if self._qa2d_generator is None:\n            if self.verbose: print(\"Initializing QA2D generator...\")\n            self._qa2d_generator = QA2D(device=self.device, verbose=self.verbose)\n        return self._qa2d_generator\n\n    @property\n    def qa_generator(self):\n        if self._qa_generator is None:\n            if self.verbose: print(\"Initializing QAnswering generator...\")\n            self._qa_generator = QAnswering(device=self.device, verbose=self.verbose)\n        return self._qa_generator\n        \n    def get_mlm_generator(self, corpus_name_key, corpra):\n        if not corpra: # If the corpus to process is empty, don't init/return dummy\n            if self.verbose: print(f\"MLM Generator not created for {corpus_name_key} due to empty corpus.\")\n            return None # Or a dummy generator that returns empty lists\n        if corpus_name_key not in self._mlm_generators:\n            if self.verbose: print(f\"Initializing MLM generator for {corpus_name_key} (Corpus size: {len(corpra)})\")\n            self._mlm_generators[corpus_name_key] = MLMGeneratorWithPairedData(\n                corpra=corpra, device=self.device, batch_size=64, mask_percent=0.25, verbose=self.verbose\n            )\n        return self._mlm_generators[corpus_name_key]\n        \n    @property\n    def ext_summarizer(self):\n        if self._ext_summarizer is None:\n            if self.verbose: print(\"Initializing ExtractiveSummarizationGenerator...\")\n            self._ext_summarizer = ExtractiveSummarizationGenerator()\n        return self._ext_summarizer\n\n    def load_dataset_from_huggingface(self):\n        for each_dataset_name in self.dataset_names:\n            if each_dataset_name not in DATASET_HUGGINGFACE:\n                if self.verbose: print(f\"Skipping {each_dataset_name}: Not in DATASET_HUGGINGFACE config.\")\n                self.datasets[each_dataset_name] = None\n                continue\n            \n            if self.verbose: print(f\"Loading raw dataset: {each_dataset_name}\")\n            config_entry = DATASET_CONFIG.get(each_dataset_name, {}) # Get config for this dataset\n            if not config_entry or not config_entry.get('huggingface'): # Ensure it's configured for HF\n                if self.verbose: print(f\"Skipping {each_dataset_name} as it's not configured for Hugging Face loading (problem in DATASET_CONFIG).\")\n                self.datasets[each_dataset_name] = None\n                continue\n\n            try:\n                dataset_args = DATASET_HUGGINGFACE[each_dataset_name][:-1]\n                split_name = DATASET_HUGGINGFACE[each_dataset_name][-1]\n                \n                hf_config_name = dataset_args[1] if len(dataset_args) > 1 and dataset_args[1] is not None else None\n                if hf_config_name:\n                     loaded_data = load_dataset(dataset_args[0], name=hf_config_name, split=split_name, trust_remote_code=True)\n                else:\n                     loaded_data = load_dataset(dataset_args[0], split=split_name, trust_remote_code=True)\n                \n                self.datasets[each_dataset_name] = loaded_data\n            except Exception as e:\n                if self.verbose: print(f\"Could not load {each_dataset_name} from Hugging Face: {e}\")\n                self.datasets[each_dataset_name] = None\n                continue\n            \n            if self.datasets.get(each_dataset_name):\n                 if self.verbose: print(f\"Loaded {each_dataset_name} with {len(self.datasets[each_dataset_name])} examples.\")\n            else:\n                 if self.verbose: print(f\"Failed to load or no examples for {each_dataset_name}.\")\n\n    def _get_field(self, example, field_name_or_list, default=\"\"):\n        # Simplified: assumes field_name_or_list is a string (single field name)\n        # Complex field extraction (like for RACE options, SQuAD answers) should be in specific process_ methods\n        if isinstance(field_name_or_list, list):\n            # This simplistic handling for list is likely insufficient for complex cases.\n            # Specific process_ methods should handle list fields.\n            # For example, for ROPES text_a: ['background', 'situation']\n            if field_name_or_list == ['background', 'situation']:\n                 return str(example.get('background',\"\")) + \" \" + str(example.get('situation',\"\"))\n            # Fallback for other list cases: take first element or join (needs careful thought per dataset)\n            if field_name_or_list:\n                 return str(example.get(field_name_or_list[0], default))\n            return default\n        return str(example.get(field_name_or_list, default))\n\n    def _generic_process_label_passthrough(self, dataset_name_key, create_text_c_via_mlm=False):\n        output = []\n        if dataset_name_key not in self.datasets or self.datasets[dataset_name_key] is None: \n            if self.verbose: print(f\"Dataset {dataset_name_key} not available for _generic_process_label_passthrough.\")\n            return output\n        \n        config = DATASET_CONFIG[dataset_name_key]\n        data_to_process_full = self.datasets[dataset_name_key]\n\n        num_raw_to_process = min(len(data_to_process_full), self.target_samples * 3 if self.target_samples < 50000 else self.target_samples * 2)\n        if len(data_to_process_full) > num_raw_to_process:\n            if self.verbose: print(f\"{dataset_name_key}: Selecting {num_raw_to_process} from {len(data_to_process_full)} for generic processing.\")\n            data_to_process = data_to_process_full.shuffle(seed=42).select(range(num_raw_to_process))\n        else:\n            data_to_process = data_to_process_full\n\n        text_b_originals_for_mlm = []\n        processed_records = []\n\n        for example in tqdm(data_to_process, desc=f'Constructing {dataset_name_key}', leave=False, disable=(not self.verbose)):\n            try:\n                text_a = self._get_field(example, config['text_a'])\n                text_b_orig = self._get_field(example, config['text_b'])\n                label = example[config['label']] \n                \n                if not text_a.strip() or not text_b_orig.strip(): # Skip if essential fields are empty\n                    if self.verbose: print(f\"Skipping example in {dataset_name_key} due to empty text_a or text_b_orig.\")\n                    continue\n\n                if dataset_name_key == 'vitaminc':\n                    if label == 'SUPPORTS': label = 0\n                    elif label == 'REFUTES': label = 2\n                    else: label = 1 \n                elif dataset_name_key == 'stsb' or dataset_name_key == 'sick': \n                    label = float(label) / 5.0  \n                elif dataset_name_key == 'doc_nli': \n                    if isinstance(label, str): \n                        if label.lower() == 'entailment': label = 1\n                        else: label = 0 \n                elif dataset_name_key == 'nli_fever': # Labels are already 0, 1, 2 integers\n                    label = int(label)\n                \n                processed_records.append({'text_a': text_a, 'text_b_orig': text_b_orig, 'orig_label': label})\n                if create_text_c_via_mlm:\n                    text_b_originals_for_mlm.append(text_b_orig)\n            except Exception as e:\n                if self.verbose: print(f\"Error processing an example in {dataset_name_key}: {e}. Example: {str(example)[:200]}\")\n                continue\n\n\n        hallucinated_text_b_all = []\n        if create_text_c_via_mlm and text_b_originals_for_mlm:\n            mlm_gen = self.get_mlm_generator(f\"{dataset_name_key}_text_b_generic\", text_b_originals_for_mlm)\n            if mlm_gen:\n                hallucinated_text_b_all = mlm_gen.generate()\n            else: # MLM generator failed to init (e.g. empty corpus)\n                hallucinated_text_b_all = [s + \" (placeholder_c)\" for s in text_b_originals_for_mlm]\n\n\n        for i, record in enumerate(processed_records):\n            current_text_c = []\n            if create_text_c_via_mlm:\n                if hallucinated_text_b_all and i < len(hallucinated_text_b_all) and hallucinated_text_b_all[i]:\n                    current_text_c.append(hallucinated_text_b_all[i])\n                else: # Fallback if MLM produced empty or list was short\n                     current_text_c.append(record['text_b_orig'] + \" (placeholder_c)\")\n\n            output.append({\n                'text_a': record['text_a'],\n                'text_b': [record['text_b_orig']],\n                'text_c': current_text_c,\n                'orig_label': record['orig_label']\n            })\n        return output\n\n    def process_mnli(self): return self._generic_process_label_passthrough('mnli', create_text_c_via_mlm=True)\n    def process_anli_r1(self): return self._generic_process_label_passthrough('anli_r1', create_text_c_via_mlm=True)\n    def process_anli_r2(self): return self._generic_process_label_passthrough('anli_r2', create_text_c_via_mlm=True)\n    def process_anli_r3(self): return self._generic_process_label_passthrough('anli_r3', create_text_c_via_mlm=True)\n    def process_snli(self): return self._generic_process_label_passthrough('snli', create_text_c_via_mlm=True)\n    def process_doc_nli(self): return self._generic_process_label_passthrough('doc_nli', create_text_c_via_mlm=True)\n    def process_vitaminc(self): return self._generic_process_label_passthrough('vitaminc', create_text_c_via_mlm=True)\n    def process_nli_fever(self): return self._generic_process_label_passthrough('nli_fever', create_text_c_via_mlm=True)\n    def process_paws(self): return self._generic_process_label_passthrough('paws', create_text_c_via_mlm=True)\n    def process_mrpc(self): return self._generic_process_label_passthrough('mrpc', create_text_c_via_mlm=True)\n    def process_qqp(self): return self._generic_process_label_passthrough('qqp', create_text_c_via_mlm=True)\n    def process_stsb(self): return self._generic_process_label_passthrough('stsb', create_text_c_via_mlm=False) \n    def process_sick(self): return self._generic_process_label_passthrough('sick', create_text_c_via_mlm=False) \n\n    def _process_summarization_generic(self, dataset_name_key):\n        output = []\n        if dataset_name_key not in self.datasets or self.datasets[dataset_name_key] is None:\n            if self.verbose: print(f\"Dataset {dataset_name_key} not loaded or empty, skipping summarization processing.\")\n            return output\n\n        config = DATASET_CONFIG[dataset_name_key]\n        original_data = self.datasets[dataset_name_key]\n        \n        num_to_process_for_mlm_source = min(len(original_data), self.target_samples * 2) \n        if len(original_data) > num_to_process_for_mlm_source:\n            if self.verbose: print(f\"{dataset_name_key}: Selecting {num_to_process_for_mlm_source} from {len(original_data)} for creating MLM source material.\")\n            data_for_mlm_source = original_data.shuffle(seed=42).select(range(num_to_process_for_mlm_source))\n        else:\n            data_for_mlm_source = original_data\n            \n        if self.verbose: print(f\"{dataset_name_key}: Using {len(data_for_mlm_source)} samples as source for MLM text_c generation.\")\n\n        texts_a_source = [self._get_field(ex, config['text_a']) for ex in data_for_mlm_source]\n        gold_summaries_source = [self._get_field(ex, config['text_b']) for ex in data_for_mlm_source]\n        \n        # Filter out empty summaries before MLM to prevent errors and save computation\n        valid_indices = [i for i, s in enumerate(gold_summaries_source) if s.strip()]\n        texts_a_source_valid = [texts_a_source[i] for i in valid_indices]\n        gold_summaries_source_valid = [gold_summaries_source[i] for i in valid_indices]\n        \n        gold_hallucinated_batch = []\n        if gold_summaries_source_valid: \n            if self.verbose: print(f\"Generating batch MLM summaries for {dataset_name_key} (from {len(gold_summaries_source_valid)} non-empty gold summaries)...\")\n            mlm_gen_gold_batch = self.get_mlm_generator(f\"{dataset_name_key}_gold_summ_batch\", gold_summaries_source_valid)\n            if mlm_gen_gold_batch:\n                gold_hallucinated_batch = mlm_gen_gold_batch.generate()\n            else: # MLM gen was not created (e.g. empty corpus after filtering)\n                gold_hallucinated_batch = [s + \" (placeholder_c)\" for s in gold_summaries_source_valid]\n        else:\n            if self.verbose: print(f\"No non-empty gold summaries to process with MLM for {dataset_name_key}.\")\n\n        # Map hallucinated summaries back to original positions, or use placeholder\n        hallucinated_map = {gold_summaries_source_valid[i]: gold_hallucinated_batch[i] for i in range(len(gold_summaries_source_valid)) if i < len(gold_hallucinated_batch)}\n\n        for i, example_data_point in tqdm(enumerate(data_for_mlm_source), \n                                           desc=f\"Constructing final {dataset_name_key} examples\", \n                                           total=len(data_for_mlm_source), \n                                           leave=False, disable=(not self.verbose)):\n            text_a_i = texts_a_source[i] \n            current_gold_summary = gold_summaries_source[i] \n\n            current_text_b = [current_gold_summary] \n            current_text_c = []\n\n            if not current_gold_summary.strip(): # Handle original empty summary\n                current_text_c.append(\"(original summary was empty - placeholder_c)\")\n            else:\n                hallucinated_version = hallucinated_map.get(current_gold_summary)\n                if hallucinated_version and hallucinated_version.strip():\n                    current_text_c.append(hallucinated_version)\n                else: \n                    if self.verbose: print(f\"Fallback: Single MLM for item {i} in {dataset_name_key} (gold summary: '{current_gold_summary[:50]}...') because batch MLM was missing or empty.\")\n                    mlm_gen_single = self.get_mlm_generator(f\"{dataset_name_key}_single_gold_fb_{i % 100}\", [current_gold_summary])\n                    if mlm_gen_single:\n                        single_hallucinated_list = mlm_gen_single.generate() \n                        if single_hallucinated_list and single_hallucinated_list[0] and single_hallucinated_list[0].strip(): \n                            current_text_c = [single_hallucinated_list[0]] \n                        else:\n                            current_text_c = [current_gold_summary + \" (placeholder_c)\"] \n                            if self.verbose: print(f\"Fallback MLM for item {i} in {dataset_name_key} also failed or returned empty. Using placeholder for text_c.\")\n                    else: # mlm_gen_single was None (e.g. empty current_gold_summary passed to get_mlm_generator)\n                        current_text_c = [current_gold_summary + \" (placeholder_c)\"]\n\n            output.append({\n                'text_a': text_a_i,\n                'text_b': current_text_b, \n                'text_c': current_text_c, \n                'orig_label': -1 \n            })\n            \n        return output\n\n    def process_xsum(self): return self._process_summarization_generic('xsum')\n    def process_cnndm(self): return self._process_summarization_generic('cnndm')\n    def process_wikihow(self): return self._process_summarization_generic('wikihow')\n    \n    def process_squad_v2_new(self): \n        dataset_name_key = 'squad_v2' \n        output = []\n        if dataset_name_key not in self.datasets or self.datasets[dataset_name_key] is None: return output\n        \n        data_to_process_full = self.datasets[dataset_name_key]\n        num_to_process = min(len(data_to_process_full), self.target_samples * 2) \n        if len(data_to_process_full) > num_to_process:\n            if self.verbose: print(f\"{dataset_name_key}: Selecting {num_to_process} from {len(data_to_process_full)} for QA processing.\")\n            data_to_process = data_to_process_full.shuffle(seed=42).select(range(num_to_process))\n        else:\n            data_to_process = data_to_process_full\n            if self.verbose: print(f\"{dataset_name_key}: Processing all {len(data_to_process)} samples for QA.\")\n\n        not_ans_contexts, not_ans_questions = [], []\n        ans_contexts, ans_questions, ans_answers = [], [], []\n\n        for ex in tqdm(data_to_process, desc=f\"Collecting SQuADv2 examples\", leave=False, disable=(not self.verbose)):\n            context = self._get_field(ex, DATASET_CONFIG[dataset_name_key]['text_a'])\n            question = self._get_field(ex, DATASET_CONFIG[dataset_name_key]['text_b'][0])\n            answers_texts = ex['answers']['text'] if 'answers' in ex and 'text' in ex['answers'] else []\n\n            if not answers_texts: \n                not_ans_contexts.append(context)\n                not_ans_questions.append(question)\n            else: \n                ans_contexts.append(context)\n                ans_questions.append(question)\n                ans_answers.append(answers_texts[0]) \n        \n        if ans_questions: # Check if list is not empty\n            ans_declarative = self.qa2d_generator.generate(ans_questions, ans_answers)\n            for i, decl_sent in enumerate(ans_declarative):\n                output.append({'text_a': ans_contexts[i], 'text_b': [decl_sent], 'text_c': [], 'orig_label': 1})\n\n        if not_ans_questions: # Check if list is not empty\n            gen_not_ans_answers = self.qa_generator.generate(not_ans_questions, not_ans_contexts)\n            not_ans_declarative = self.qa2d_generator.generate(not_ans_questions, gen_not_ans_answers)\n            for i, decl_sent in enumerate(not_ans_declarative):\n                output.append({'text_a': not_ans_contexts[i], 'text_b': [decl_sent], 'text_c': [], 'orig_label': 0})\n        return output\n\n    def process_race(self):\n        dataset_name_key = 'race'\n        output = []\n        if dataset_name_key not in self.datasets or self.datasets[dataset_name_key] is None: return output\n        \n        config = DATASET_CONFIG[dataset_name_key]\n        data_to_process_full = self.datasets[dataset_name_key]\n        num_to_process = min(len(data_to_process_full), self.target_samples * 2)\n        if len(data_to_process_full) > num_to_process:\n            if self.verbose: print(f\"{dataset_name_key}: Selecting {num_to_process} from {len(data_to_process_full)} for QA processing.\")\n            data_to_process = data_to_process_full.shuffle(seed=42).select(range(num_to_process))\n        else:\n            data_to_process = data_to_process_full\n\n        option_dict = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n        \n        # For QA2D batching\n        contexts_for_qa2d_cloze, questions_for_qa2d_cloze, labels_for_cloze = [],[],[]\n        contexts_for_qa2d_std, questions_for_qa2d_std_q, answers_for_qa2d_std_correct_opt, answers_for_qa2d_std_wrong_opt = [], [], [], []\n\n\n        for ex in tqdm(data_to_process, desc=f\"Constructing {dataset_name_key}\", leave=False, disable=(not self.verbose)):\n            article = self._get_field(ex, config['text_a'])\n            question = self._get_field(ex, config['text_b'][0])\n            options = ex[config['text_b'][1]] \n            answer_char = ex[config['label']]\n            answer_idx = option_dict.get(answer_char, -1)\n            if answer_idx == -1: continue\n\n            if \"_\" in question: \n                for i, opt_text in enumerate(options):\n                    filled_question = question.replace(\"_\", \" \" + opt_text + \" \").strip()\n                    # Direct output for cloze-style, QA2D not typically used here by AlignScore logic\n                    output.append({\n                        'text_a': article,\n                        'text_b': [filled_question], \n                        'text_c': [], \n                        'orig_label': 1 if i == answer_idx else 0\n                    })\n            else: \n                # Collect for batch QA2D\n                contexts_for_qa2d_std.append(article)\n                questions_for_qa2d_std_q.append(question)\n                answers_for_qa2d_std_correct_opt.append(options[answer_idx])\n                \n                current_wrong_options = [opt for i, opt in enumerate(options) if i != answer_idx and opt]\n                if current_wrong_options:\n                    # Need to append context and question again for the negative pair\n                    contexts_for_qa2d_std.append(article) \n                    questions_for_qa2d_std_q.append(question)\n                    answers_for_qa2d_std_wrong_opt.append(random.choice(current_wrong_options))\n                # If no wrong options, only positive will be generated by QA2D from correct_options list\n\n        # Batch process standard QA style with QA2D\n        if questions_for_qa2d_std_q:\n            # Separate positive and negative based on how they were collected\n            # This assumes an alternating or paired collection for contexts_for_qa2d_std\n            # Let's refine: generate positive and negative declarative sentences separately\n            \n            # Positive pairs for QA2D\n            temp_questions_pos = []\n            temp_answers_pos = []\n            temp_contexts_pos = []\n            \n            # Negative pairs for QA2D\n            temp_questions_neg = []\n            temp_answers_neg = []\n            temp_contexts_neg = []\n\n            # Re-iterate to correctly pair for QA2D generation\n            # This is inefficient, better to collect for QA2D in the main loop if structure is complex\n            # Simplified: Assuming we create positive and one random negative for each original QA item\n            \n            # For simplicity, let's re-process data_to_process for QA-style questions for QA2D\n            qa_style_items_for_qa2d = []\n            for ex in data_to_process: # Iterate again for QA-style items for clarity\n                if \"_\" not in self._get_field(ex, config['text_b'][0]):\n                    article = self._get_field(ex, config['text_a'])\n                    question = self._get_field(ex, config['text_b'][0])\n                    options = ex[config['text_b'][1]]\n                    answer_idx = option_dict.get(ex[config['label']],-1)\n                    if answer_idx == -1: continue\n                    \n                    qa_style_items_for_qa2d.append({\n                        'context': article, 'question': question, \n                        'correct_ans': options[answer_idx],\n                        'wrong_ans_options': [opt for i, opt in enumerate(options) if i != answer_idx and opt]\n                    })\n\n            if qa_style_items_for_qa2d:\n                q_pos = [item['question'] for item in qa_style_items_for_qa2d]\n                ans_pos = [item['correct_ans'] for item in qa_style_items_for_qa2d]\n                ctx_pos = [item['context'] for item in qa_style_items_for_qa2d]\n                decl_positive_qa = self.qa2d_generator.generate(q_pos, ans_pos)\n                for i, decl_s in enumerate(decl_positive_qa):\n                    output.append({'text_a': ctx_pos[i], 'text_b': [decl_s], 'text_c': [], 'orig_label': 1})\n\n                q_neg = [item['question'] for item in qa_style_items_for_qa2d if item['wrong_ans_options']]\n                ans_neg = [random.choice(item['wrong_ans_options']) for item in qa_style_items_for_qa2d if item['wrong_ans_options']]\n                ctx_neg = [item['context'] for item in qa_style_items_for_qa2d if item['wrong_ans_options']]\n                if q_neg: # If there are questions for negative examples\n                    decl_negative_qa = self.qa2d_generator.generate(q_neg, ans_neg)\n                    for i, decl_s in enumerate(decl_negative_qa):\n                        output.append({'text_a': ctx_neg[i], 'text_b': [decl_s], 'text_c': [], 'orig_label': 0})\n        return output\n        \n    def _process_qa_multiple_choice_options(self, dataset_name_key):\n        output = []\n        if dataset_name_key not in self.datasets or self.datasets[dataset_name_key] is None:\n            if self.verbose: print(f\"Dataset {dataset_name_key} not loaded or empty. Skipping in _process_qa_multiple_choice_options.\")\n            return output\n\n        config = DATASET_CONFIG[dataset_name_key]\n        data_to_process_full = self.datasets[dataset_name_key]\n        \n        num_raw_to_process = min(len(data_to_process_full), self.target_samples * 2)\n        if len(data_to_process_full) > num_raw_to_process:\n            if self.verbose: print(f\"{dataset_name_key}: Selecting {num_raw_to_process} from {len(data_to_process_full)} for initial processing.\")\n            data_to_process = data_to_process_full.shuffle(seed=42).select(range(num_raw_to_process))\n        else:\n            data_to_process = data_to_process_full\n\n        # Lists for batch QA2D\n        contexts_for_batch_qa2d = []\n        questions_for_batch_qa2d = []\n        answers_for_batch_qa2d_positive = []\n        answers_for_batch_qa2d_negative = []\n        # Keep track of original context for pairing later\n        original_contexts_for_positive = [] \n        original_contexts_for_negative = []\n\n\n        for ex in tqdm(data_to_process, desc=f\"Preparing pairs for {dataset_name_key}\", leave=False, disable=(not self.verbose)):\n            context_str = self._get_field(ex, config['text_a'])\n            # For boolq, text_b is just question. For sciq/quail, text_b is list of [question, ans_options...]\n            question_str = self._get_field(ex, config['text_b'][0] if isinstance(config['text_b'], list) else config['text_b'])\n            \n            is_boolq = (dataset_name_key == 'boolq')\n            correct_option_text = \"\"\n            incorrect_option_choices = []\n\n            if is_boolq:\n                true_answer_bool = ex[config['label']] # 'answer' field for boolq\n                correct_option_text = \"Yes.\" if true_answer_bool else \"No.\"\n                incorrect_option_choices = [\"No.\" if true_answer_bool else \"Yes.\"]\n            elif dataset_name_key == 'quail':\n                options = ex.get('answers', []) \n                correct_answer_id = ex.get('correct_answer_id', -1)\n                if 0 <= correct_answer_id < len(options):\n                    correct_option_text = options[correct_answer_id]\n                    incorrect_option_choices = [opt for i, opt in enumerate(options) if i != correct_answer_id and opt]\n                else: continue # Skip malformed\n            elif dataset_name_key == 'sciq':\n                correct_option_text = self._get_field(ex, config['text_b'][1]) # correct_answer\n                incorrect_option_choices = [\n                    self._get_field(ex, config['text_b'][2]), # distractor1\n                    self._get_field(ex, config['text_b'][3]), # distractor2\n                    self._get_field(ex, config['text_b'][4]), # distractor3\n                ]\n                incorrect_option_choices = [opt for opt in incorrect_option_choices if opt] # Filter empty distractors\n            \n            if not correct_option_text: continue # Skip if no correct option\n\n            # Add positive pair for QA2D\n            contexts_for_batch_qa2d.append(context_str) # Will be duplicated if also making negative from same context\n            questions_for_batch_qa2d.append(question_str)\n            answers_for_batch_qa2d_positive.append(correct_option_text)\n            original_contexts_for_positive.append(context_str)\n\n            # Add one negative pair for QA2D\n            if incorrect_option_choices:\n                contexts_for_batch_qa2d.append(context_str) # Same context, different answer\n                questions_for_batch_qa2d.append(question_str)\n                answers_for_batch_qa2d_negative.append(random.choice(incorrect_option_choices))\n                original_contexts_for_negative.append(context_str)\n        \n        # Batch QA2D conversion\n        if answers_for_batch_qa2d_positive:\n            # Generate positive declarative sentences\n            # Need to get the questions that correspond ONLY to positive answers\n            positive_q_list = questions_for_batch_qa2d[:len(answers_for_batch_qa2d_positive)]\n            decl_positive = self.qa2d_generator.generate(positive_q_list, answers_for_batch_qa2d_positive)\n            for i, decl_s in enumerate(decl_positive):\n                output.append({'text_a': original_contexts_for_positive[i], 'text_b': [decl_s], 'text_c':[], 'orig_label': 1})\n        \n        if answers_for_batch_qa2d_negative:\n            # Generate negative declarative sentences\n            # Questions for negative answers start after questions for positive ones in the batched list\n            negative_q_list = questions_for_batch_qa2d[len(answers_for_batch_qa2d_positive):]\n            decl_negative = self.qa2d_generator.generate(negative_q_list, answers_for_batch_qa2d_negative)\n            for i, decl_s in enumerate(decl_negative):\n                output.append({'text_a': original_contexts_for_negative[i], 'text_b': [decl_s], 'text_c':[], 'orig_label': 0})\n        \n        if self.verbose: print(f\"Finished _process_qa_multiple_choice_options for {dataset_name_key}. Generated {len(output)} examples.\")\n        return output\n\n    def process_quail(self): return self._process_qa_multiple_choice_options('quail')\n    def process_sciq(self): return self._process_qa_multiple_choice_options('sciq')\n    def process_boolq(self): return self._process_qa_multiple_choice_options('boolq')\n\n    def _process_qa_extractive_or_abstractive(self, dataset_name_key, negative_sample_no_ans_only=True):\n        output = []\n        if dataset_name_key not in self.datasets or self.datasets[dataset_name_key] is None: return output\n        \n        config = DATASET_CONFIG[dataset_name_key]\n        data_to_process_full = self.datasets[dataset_name_key]\n        num_to_process = min(len(data_to_process_full), self.target_samples * 2) \n        if len(data_to_process_full) > num_to_process:\n            if self.verbose: print(f\"{dataset_name_key}: Selecting {num_to_process} from {len(data_to_process_full)} for QA processing.\")\n            data_to_process = data_to_process_full.shuffle(seed=42).select(range(num_to_process))\n        else:\n            data_to_process = data_to_process_full\n            \n        samples_for_processing = [] \n\n        if dataset_name_key == 'newsqa': \n            for story in tqdm(data_to_process, desc=f'Collecting {dataset_name_key} examples', disable=(not self.verbose)):\n                # newsqa HF dataset is already flat, not like the raw JSON structure\n                context = self._get_field(story, 'story_text') # Assuming HF 'newsqa' structure\n                question_text = self._get_field(story, 'question_text')\n                current_answers = []\n                # Newsqa HF dataset has 'answer_char_ranges' which might be tricky if multiple answers.\n                # For simplicity, if it provides a direct answer string, use that.\n                # The original AlignScore parsed raw JSON 'consensus'.\n                # Simplified: if 'answer_token_ranges' indicates an answer. Assume one for now.\n                # This part needs careful check against actual HF 'newsqa' features.\n                # Placeholder: Assume we can get an answer string if available.\n                # If not, current_answers remains empty.\n                # This simplification might lead to many \"no answer\" cases for newsqa if features are complex.\n                # A more robust parser for newsqa's specific answer format is needed.\n                # For now, let's assume 'answer' is a field or deriveable.\n                # If we take the first answer like in squad:\n                # For now, we will assume it has 'answers' list like other QA datasets for simplicity\n                # This is a known simplification and might need dataset-specific code here.\n                # Let's assume `story` has a field that can be treated as `answers['text']` like other QAs.\n                # This is a placeholder section that likely needs real newsqa parsing logic.\n                # For now, we'll use a dummy logic to avoid crashing\n                # current_answers = [ans_text for ans_text in story.get('answers',{}).get('text',[]) if ans_text.strip()]\n                # Given newsqa's complexity, it's better to have a dedicated parser or simplify greatly.\n                # For now, this method might not produce good newsqa data.\n                # Let's assume `text_b` config for newsqa is simplified for now.\n                # If DATASET_CONFIG['newsqa']['text_b'] = ['question', 'answer_text_field_if_exists']\n                answer_texts = [] # This needs to be properly extracted from newsqa's format\n                if 'answer_token_ranges' in story and story['answer_token_ranges'] and story['answer_token_ranges'][0]['start'] != -1:\n                     # This is complex as it requires token mapping.\n                     # For a simpler demo, if a flat answer text field were available, it'd be easier.\n                     # Skipping complex answer extraction for newsqa in this generic function.\n                     pass # This means newsqa will mostly generate \"no answer\" scenarios here.\n                samples_for_processing.append((context, question_text, answer_texts))\n\n        else: \n            for ex in tqdm(data_to_process, desc=f\"Collecting {dataset_name_key} examples\", disable=(not self.verbose)):\n                context = \"\"\n                if isinstance(config['text_a'], list): \n                    context = self._get_field(ex, config['text_a'][0], \"\") + \" \" + self._get_field(ex, config['text_a'][1], \"\")\n                else:\n                    context = self._get_field(ex, config['text_a'])\n\n                question = self._get_field(ex, config['text_b'][0])\n                \n                answers_field_name = config['text_b'][1]\n                raw_answers_data = ex.get(answers_field_name)\n                current_answers_text_list = []\n                if isinstance(raw_answers_data, dict) and 'text' in raw_answers_data: \n                    ans_text = raw_answers_data['text']\n                    current_answers_text_list = ans_text if isinstance(ans_text, list) else [ans_text]\n                elif isinstance(raw_answers_data, dict) and 'spans' in raw_answers_data: # DROP\n                    ans_spans = raw_answers_data['spans']\n                    current_answers_text_list = ans_spans if isinstance(ans_spans, list) else [ans_spans]\n                elif isinstance(raw_answers_data, list): # Duorc, ROPES answers\n                     current_answers_text_list = raw_answers_data\n                \n                current_answers_text_list = [str(ans).strip() for ans in current_answers_text_list if str(ans).strip()]\n                samples_for_processing.append((context, question, current_answers_text_list))\n\n        contexts_pos, questions_pos, answers_pos_text = [], [], []\n        contexts_for_neg_gen, questions_for_neg_gen, gold_answers_for_neg_gen_ctx = [], [], []\n\n        for context, question, gold_answers_list in samples_for_processing:\n            if gold_answers_list: \n                contexts_pos.append(context)\n                questions_pos.append(question)\n                answers_pos_text.append(random.choice(gold_answers_list)) \n            \n            if not gold_answers_list or not negative_sample_no_ans_only:\n                contexts_for_neg_gen.append(context)\n                questions_for_neg_gen.append(question)\n                gold_answers_for_neg_gen_ctx.append(gold_answers_list) \n\n        if questions_pos: # If there are positive examples to convert\n            decl_positive = self.qa2d_generator.generate(questions_pos, answers_pos_text)\n            for i, decl_s in enumerate(decl_positive):\n                output.append({'text_a': contexts_pos[i], 'text_b': [decl_s], 'text_c':[], 'orig_label': 1})\n        \n        if questions_for_neg_gen: # If there are examples for negative generation\n            # Here, QAnswering generates an answer, then QA2D converts Q+GeneratedA to declarative\n            generated_answers_for_neg_pairs = self.qa_generator.generate(questions_for_neg_gen, contexts_for_neg_gen)\n            decl_negative = self.qa2d_generator.generate(questions_for_neg_gen, generated_answers_for_neg_pairs)\n            for i, decl_s in enumerate(decl_negative):\n                output.append({'text_a': contexts_for_neg_gen[i], 'text_b': [decl_s], 'text_c':[], 'orig_label': 0})\n        return output\n\n    def process_adversarial_qa(self): return self._process_qa_extractive_or_abstractive('adversarial_qa', negative_sample_no_ans_only=False)\n    def process_drop(self): return self._process_qa_extractive_or_abstractive('drop', negative_sample_no_ans_only=False)\n    def process_duorc_self(self): return self._process_qa_extractive_or_abstractive('duorc_self', negative_sample_no_ans_only=False)\n    def process_duorc_paraphrase(self): return self._process_qa_extractive_or_abstractive('duorc_paraphrase', negative_sample_no_ans_only=False)\n    def process_quoref(self): return self._process_qa_extractive_or_abstractive('quoref', negative_sample_no_ans_only=False)\n    def process_ropes(self): return self._process_qa_extractive_or_abstractive('ropes', negative_sample_no_ans_only=False)\n    def process_newsqa(self): return self._process_qa_extractive_or_abstractive('newsqa', negative_sample_no_ans_only=False)\n\n    def process_hotpot_qa_generic(self, dataset_name_key):\n        output = []\n        if dataset_name_key not in self.datasets or self.datasets[dataset_name_key] is None: return output\n\n        data_to_process_full = self.datasets[dataset_name_key]\n        num_to_process = min(len(data_to_process_full), self.target_samples * 2)\n        if len(data_to_process_full) > num_to_process:\n            if self.verbose: print(f\"{dataset_name_key}: Selecting {num_to_process} from {len(data_to_process_full)} for QA processing.\")\n            data_to_process = data_to_process_full.shuffle(seed=42).select(range(num_to_process))\n        else:\n            data_to_process = data_to_process_full\n\n        contexts_pos_qa2d, questions_pos_qa2d, answers_pos_qa2d = [], [], []\n        contexts_neg_qa2d, questions_neg_qa2d, answers_neg_qa2d = [], [], []\n\n        for ex in tqdm(data_to_process, desc=f\"Preparing {dataset_name_key}\", disable=(not self.verbose)):\n            question = ex['question']\n            answer = ex['answer']\n            supporting_facts_titles = set(title for title, _ in ex.get('supporting_facts', {}).get('title', [])) # Handle missing keys\n\n            context_docs_text = []\n            irrelevant_docs_text = []\n\n            for title, sentences in zip(ex.get('context', {}).get('title', []), ex.get('context', {}).get('sentences', [])):\n                doc_text = \" \".join(sentences)\n                if title in supporting_facts_titles:\n                    context_docs_text.append(doc_text)\n                else:\n                    irrelevant_docs_text.append(doc_text)\n            \n            if not context_docs_text: # Need at least one supporting document\n                # if self.verbose: print(f\"Skipping HotpotQA example, no supporting facts text found: {question[:50]}\")\n                continue\n\n            positive_context_str = \" \".join(context_docs_text)\n            if irrelevant_docs_text and len(context_docs_text) < 3: # Add some distractors to positive context\n                positive_context_str += \" \" + \" \".join(random.sample(irrelevant_docs_text, min(len(irrelevant_docs_text), 1)))\n            \n            contexts_pos_qa2d.append(positive_context_str.strip())\n            questions_pos_qa2d.append(question)\n            answers_pos_qa2d.append(answer)\n            \n            # Create negative example: use only irrelevant context, or generate fake answer for positive context\n            if irrelevant_docs_text:\n                 negative_context_str = \" \".join(random.sample(irrelevant_docs_text, min(len(irrelevant_docs_text), 2)))\n                 fake_answer_from_neg_ctx = self.qa_generator.generate([question], [negative_context_str.strip()])[0]\n                 contexts_neg_qa2d.append(negative_context_str.strip())\n                 questions_neg_qa2d.append(question)\n                 answers_neg_qa2d.append(fake_answer_from_neg_ctx)\n            # Fallback: generate a (hopefully different) answer from the positive context\n            fake_answer_for_pos_ctx = self.qa_generator.generate([question], [positive_context_str.strip()])[0]\n            if fake_answer_for_pos_ctx.lower().strip() != answer.lower().strip():\n                 contexts_neg_qa2d.append(positive_context_str.strip())\n                 questions_neg_qa2d.append(question)\n                 answers_neg_qa2d.append(fake_answer_for_pos_ctx)\n\n        if questions_pos_qa2d:\n            decl_positive = self.qa2d_generator.generate(questions_pos_qa2d, answers_pos_qa2d)\n            for i, decl_s in enumerate(decl_positive):\n                output.append({'text_a': contexts_pos_qa2d[i], 'text_b': [decl_s], 'text_c':[], 'orig_label': 1})\n\n        if questions_neg_qa2d:\n            decl_negative = self.qa2d_generator.generate(questions_neg_qa2d, answers_neg_qa2d)\n            for i, decl_s in enumerate(decl_negative):\n                output.append({'text_a': contexts_neg_qa2d[i], 'text_b': [decl_s], 'text_c':[], 'orig_label': 0})\n        return output\n\n    def process_hotpot_qa_distractor(self): return self.process_hotpot_qa_generic('hotpot_qa_distractor')\n    def process_hotpot_qa_fullwiki(self): return self.process_hotpot_qa_generic('hotpot_qa_fullwiki')\n\n    def process_eraser_multi_rc(self):\n        dataset_name_key = 'eraser_multi_rc'\n        output = []\n        if dataset_name_key not in self.datasets or self.datasets[dataset_name_key] is None: return output\n        \n        config = DATASET_CONFIG[dataset_name_key]\n        data_to_process_full = self.datasets[dataset_name_key]\n        num_to_process = min(len(data_to_process_full), self.target_samples * 2) \n        if len(data_to_process_full) > num_to_process:\n            if self.verbose: print(f\"{dataset_name_key}: Selecting {num_to_process} from {len(data_to_process_full)} for processing.\")\n            data_to_process = data_to_process_full.shuffle(seed=42).select(range(num_to_process))\n        else:\n            data_to_process = data_to_process_full\n        \n        processed_records = []\n        texts_b_for_mlm = []\n\n        for ex in tqdm(data_to_process, desc=f\"Constructing {dataset_name_key}\", disable=(not self.verbose)):\n            text_a = self._get_field(ex, config['text_a'])\n            text_b_orig = self._get_field(ex, config['text_b']).replace(\"|\", \"\").strip()\n            label = int(ex[config['label']])\n            \n            if not text_a or not text_b_orig: continue\n\n            processed_records.append({'text_a': text_a, 'text_b_orig': text_b_orig, 'orig_label': label})\n            if label == 1: # Only create perturbed negatives for positive examples\n                texts_b_for_mlm.append(text_b_orig)\n            else: # For existing negative examples, we won't add a text_c via MLM\n                texts_b_for_mlm.append(None) # Placeholder\n\n        hallucinated_text_b_list = [None] * len(processed_records)\n        # Collect only non-None texts for MLM\n        actual_texts_to_mlm = [t for t in texts_b_for_mlm if t is not None]\n        if actual_texts_to_mlm:\n            mlm_gen = self.get_mlm_generator(f\"{dataset_name_key}_text_b_conditional\", actual_texts_to_mlm)\n            if mlm_gen:\n                hallucinated_results = mlm_gen.generate()\n                # Map results back\n                hall_idx = 0\n                for i in range(len(texts_b_for_mlm)):\n                    if texts_b_for_mlm[i] is not None:\n                        if hall_idx < len(hallucinated_results):\n                            hallucinated_text_b_list[i] = hallucinated_results[hall_idx]\n                            hall_idx += 1\n                        else: # Should not happen if generate returns full list\n                            hallucinated_text_b_list[i] = texts_b_for_mlm[i] + \" (placeholder_c)\"\n\n\n        for i, record in enumerate(processed_records):\n            current_text_c = []\n            if record['orig_label'] == 1 and hallucinated_text_b_list[i]: # Add MLM'd version as text_c for positive examples\n                current_text_c.append(hallucinated_text_b_list[i])\n            \n            output.append({\n                'text_a': record['text_a'],\n                'text_b': [record['text_b_orig']],\n                'text_c': current_text_c, \n                'orig_label': record['orig_label']\n            })\n        return output\n        \n    def process_strategy_qa(self):\n        dataset_name_key = 'strategy_qa'\n        output = []\n        if dataset_name_key not in self.datasets or self.datasets[dataset_name_key] is None: return output\n\n        config = DATASET_CONFIG[dataset_name_key]\n        data_to_process_full = self.datasets[dataset_name_key]\n        num_to_process = min(len(data_to_process_full), self.target_samples * 2)\n        if len(data_to_process_full) > num_to_process:\n            if self.verbose: print(f\"{dataset_name_key}: Selecting {num_to_process} from {len(data_to_process_full)} for QA processing.\")\n            data_to_process = data_to_process_full.shuffle(seed=42).select(range(num_to_process))\n        else:\n            data_to_process = data_to_process_full\n\n        contexts_batch, questions_batch, answers_pos_batch, answers_neg_batch = [], [], [], []\n\n        for ex in tqdm(data_to_process, desc=f\"Constructing {dataset_name_key}\", disable=(not self.verbose)):\n            facts_list = ex.get(config['text_a'], []) # 'facts' can be a list\n            context = \" \".join(facts_list) if isinstance(facts_list, list) else str(facts_list)\n            question = self._get_field(ex, config['text_b'])\n            answer_bool = ex[config['label']] # True or False in HF dataset\n\n            if not context.strip() or not question.strip(): continue\n\n            contexts_batch.append(context)\n            questions_batch.append(question)\n            answers_pos_batch.append(\"Yes.\" if answer_bool else \"No.\")\n            \n            # For negative pair\n            contexts_batch.append(context) \n            questions_batch.append(question) \n            answers_neg_batch.append(\"No.\" if answer_bool else \"Yes.\")\n\n        # Batch QA2D for positive framed answers\n        if questions_batch[:len(answers_pos_batch)]: # Ensure lists are not empty\n            decl_pos = self.qa2d_generator.generate(questions_batch[:len(answers_pos_batch)], answers_pos_batch)\n            for i, decl_s in enumerate(decl_pos):\n                output.append({'text_a': contexts_batch[i*2], 'text_b': [decl_s], 'text_c': [], 'orig_label': 1})\n        \n        # Batch QA2D for negative framed answers\n        if questions_batch[len(answers_pos_batch):]: # Ensure lists are not empty\n            decl_neg = self.qa2d_generator.generate(questions_batch[len(answers_pos_batch):], answers_neg_batch)\n            for i, decl_s in enumerate(decl_neg):\n                output.append({'text_a': contexts_batch[i*2+1], 'text_b': [decl_s], 'text_c': [], 'orig_label': 0})\n        return output\n        \n    def process_gap(self):\n        dataset_name_key = 'gap'\n        output = []\n        if dataset_name_key not in self.datasets or self.datasets[dataset_name_key] is None: return output\n\n        config = DATASET_CONFIG[dataset_name_key]\n        data_to_process_full = self.datasets[dataset_name_key]\n        num_to_process = min(len(data_to_process_full), self.target_samples * 2)\n        if len(data_to_process_full) > num_to_process:\n            if self.verbose: print(f\"{dataset_name_key}: Selecting {num_to_process} from {len(data_to_process_full)} for Coref processing.\")\n            data_to_process = data_to_process_full.shuffle(seed=42).select(range(num_to_process))\n        else:\n            data_to_process = data_to_process_full\n\n        for ex in tqdm(data_to_process, desc=f\"Constructing {dataset_name_key}\", disable=(not self.verbose)):\n            text_full = self._get_field(ex, config['text_a']) # 'Text'\n            pronoun = self._get_field(ex, config['text_b'][0]) # 'Pronoun'\n            entity_A_text = self._get_field(ex, config['text_b'][1]) # 'A'\n            entity_B_text = self._get_field(ex, config['text_b'][2]) # 'B'\n            \n            pronoun_offset = ex.get('Pronoun-offset', -1) # Check if field exists\n            if pronoun_offset == -1: \n                if self.verbose: print(f\"Skipping GAP example due to missing 'Pronoun-offset': {text_full[:50]}\")\n                continue\n\n            # Ensure pronoun matches to avoid errors if offset is slightly off or pronoun casing differs\n            actual_pronoun_in_text = text_full[pronoun_offset : pronoun_offset + len(pronoun)]\n            if actual_pronoun_in_text.lower() != pronoun.lower():\n                # Try finding the pronoun if offset is wrong; this is a bit risky\n                try:\n                    pronoun_offset = text_full.lower().index(pronoun.lower(), pronoun_offset - 5, pronoun_offset + 5)\n                except ValueError:\n                    if self.verbose: print(f\"Skipping GAP example, pronoun '{pronoun}' not found near offset {pronoun_offset}: {text_full[:100]}\")\n                    continue\n\n\n            try:\n                text_with_A = text_full[:pronoun_offset] + entity_A_text + text_full[pronoun_offset + len(pronoun):]\n                label_A = 1 if ex[config['label'][0]] else 0 # A-coref (True/False)\n                output.append({'text_a': text_full, 'text_b': [text_with_A], 'text_c': [], 'orig_label': label_A})\n\n                text_with_B = text_full[:pronoun_offset] + entity_B_text + text_full[pronoun_offset + len(pronoun):]\n                label_B = 1 if ex[config['label'][1]] else 0 # B-coref (True/False)\n                output.append({'text_a': text_full, 'text_b': [text_with_B], 'text_c': [], 'orig_label': label_B})\n            except Exception as e_gap:\n                 if self.verbose: print(f\"Error substituting pronoun for GAP example: {e_gap}. Text: {text_full[:100]}\")\n                 continue\n        return output\n\n    def generate(self): \n        all_processed_data_info = {} \n        for each_dataset_name in self.dataset_names: \n            if each_dataset_name not in self.datasets or self.datasets[each_dataset_name] is None: \n                if self.verbose: print(f\"Dataset {each_dataset_name} not loaded or empty, skipping processing.\")\n                all_processed_data_info[each_dataset_name] = 0\n                continue\n\n            if self.verbose: print(f\"\\nProcessing dataset: {each_dataset_name}\")\n            \n            method_name_to_call = DATASET_CONFIG.get(each_dataset_name, {}).get('method_name', each_dataset_name)\n            \n            processed_examples = []\n            if hasattr(self, f'process_{method_name_to_call}'): \n                processed_examples = getattr(self, f'process_{method_name_to_call}')() \n            else:\n                if self.verbose: print(f\"Warning: No specific process method found for {each_dataset_name} (tried process_{method_name_to_call}). Skipping.\")\n                all_processed_data_info[each_dataset_name] = 0\n                continue\n                \n            if processed_examples:\n                if len(processed_examples) > self.target_samples: \n                    if self.verbose: print(f\"Sampling {self.target_samples} from {len(processed_examples)} for {each_dataset_name}\")\n                    processed_examples = random.sample(processed_examples, self.target_samples) \n                else:\n                    if self.verbose: print(f\"Using all {len(processed_examples)} examples for {each_dataset_name} (target was {self.target_samples})\")\n                \n                all_processed_data_info[each_dataset_name] = len(processed_examples)\n                \n                output_path = os.path.join(GENERATED_DATA_DIR, f\"{each_dataset_name}_20k.jsonl\") \n                with open(output_path, 'w', encoding='utf8') as outfile:\n                    for record in processed_examples: \n                        record_to_write = { \n                            'task': DATASET_CONFIG[each_dataset_name]['task'], \n                            'text_a': record['text_a'], \n                            'text_b': record['text_b'], \n                            'text_c': record.get('text_c', []), \n                            'orig_label': record['orig_label'] \n                        }\n                        json.dump(record_to_write, outfile, ensure_ascii=False) \n                        outfile.write('\\n') \n                if self.verbose: print(f\"Saved {len(processed_examples)} samples for {each_dataset_name} to {output_path}\")\n            else:\n                if self.verbose: print(f\"No examples processed for {each_dataset_name}.\")\n                all_processed_data_info[each_dataset_name] = 0\n        \n        return all_processed_data_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T23:23:21.122430Z","iopub.execute_input":"2025-05-28T23:23:21.123124Z","iopub.status.idle":"2025-05-28T23:23:21.252933Z","shell.execute_reply.started":"2025-05-28T23:23:21.123100Z","shell.execute_reply":"2025-05-28T23:23:21.252204Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"# Execute Data Generation and Sampling","metadata":{}},{"cell_type":"code","source":"# # Cell 3: Execute Data Generation and Sampling\n\n# # --- Specify NLI datasets to generate ---\n# # Ensure these names are keys in your DATASET_HUGGINGFACE and DATASET_CONFIG in Cell 2\n# # and that their 'task' in DATASET_CONFIG is 'nli' or 'bin_nli' (for doc_nli)\n# # or 'fact_checking' if you decide to keep nli_fever as that task type (model will use NLI head for it if label is 0,1,2)\n\n# datasets_to_generate = [\n#     # 'mnli',\n#     # 'snli',\n#     # 'anli_r1',\n#     # 'anli_r2',\n#     # 'anli_r3',\n#     'doc_nli',    # This is configured as 'bin_nli' in DATASET_CONFIG, processed by _generic_process_label_passthrough\n#     'nli_fever',  # Now configured with correct fields and 'nli' task type\n#     # Add other NLI-like datasets if you have them configured, e.g., 'vitaminc' if you consider it NLI-like\n#     # 'paws', 'qqp', 'mrpc' are 'paraphrase' but share similarities if you want binary NLI style.\n#     # 'stsb', 'sick' are 'sts' (regression).\n# ]\n\n# # Filter datasets_to_generate to only include those defined in DATASET_CONFIG \n# # (and implicitly in DATASET_HUGGINGFACE by the DataGenerator's loading logic)\n# datasets_to_generate = [name for name in datasets_to_generate if name in DATASET_CONFIG]\n\n# print(f\"Initializing DataGenerator for NLI and NLI-adjacent datasets: {datasets_to_generate}\")\n# data_gen = DataGenerator(datasets_to_generate,\n#                            target_samples_per_dataset=SAMPLES_PER_DATASET_TARGET,\n#                            verbose=True) # Set to False for less output once debugged\n\n# print(\"\\nStarting data generation and sampling for NLI datasets...\")\n# generated_data_info = data_gen.generate() \n\n# print(\"\\n--- NLI Data Generation and Sampling Complete ---\")\n# for name, num_samples in generated_data_info.items():\n#     print(f\"Dataset: {name}, Samples saved: {num_samples}\")\n\n# print(\"\\nGenerated files in:\", GENERATED_DATA_DIR)\n# # Quick check for nli_fever\n# for f_name in os.listdir(GENERATED_DATA_DIR):\n#     if f_name == \"nli_fever_20k.jsonl\":\n#         print(f\"  First few lines of {f_name}:\")\n#         with open(os.path.join(GENERATED_DATA_DIR, f_name), 'r') as f_check:\n#             for _ in range(min(5, SAMPLES_PER_DATASET_TARGET)): # Print up to 5 lines\n#                 line = f_check.readline()\n#                 if not line: break\n#                 print(f\"    {line.strip()}\")\n#         break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T21:55:14.521904Z","iopub.status.idle":"2025-05-28T21:55:14.522318Z","shell.execute_reply.started":"2025-05-28T21:55:14.522063Z","shell.execute_reply":"2025-05-28T21:55:14.522124Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# datasets_to_generate = [\n#     # Tier 1: Core NLI & Paraphrase\n#     'mnli',             \n#     'paws',             \n#     'snli',             \n#     'anli_r1',          \n#     'anli_r2',          \n#     'anli_r3',          \n#     'mrpc',             \n#     'qqp',              \n#     'doc_nli',          \n\n#     # Tier 2: STS & Fact Checking\n#     'stsb',             \n#     'sick',             \n#     'vitaminc',         \n#     'nli_fever',\n\n#     # Tier 3: Summarization (MLM can be slow)\n#     'xsum',             # Already have\n#     'cnndm',            # Add\n#     # 'wikihow',        # Add if you confirm HF 'wikihow' processing is efficient\n\n#     # Tier 4: Question Answering (Some can be very slow to process due to QA2D/QAnswering)\n#     # 'squad_v2',       # Add if you have time (uses process_squad_v2_new)\n#     # 'race',           # Add if you have time\n#     'boolq',            # Add (relatively straightforward)\n#     # 'adversarial_qa', # Add if you have time\n\n#     # Tier 5: Other\n#     # 'gap',            # Add for coreference if desired\n# ]\n\n# # Filter datasets_to_generate to only include those defined in your DATASET_CONFIG and DATASET_HUGGINGFACE in Cell 2\n# datasets_to_generate = [name for name in datasets_to_generate if name in DATASET_CONFIG and name in DATASET_HUGGINGFACE]\n\n# print(f\"Initializing DataGenerator for: {datasets_to_generate}\")\n# # Set verbose=False for less tqdm output during actual runs if preferred\n# data_gen = DataGenerator(datasets_to_generate,\n#                            target_samples_per_dataset=SAMPLES_PER_DATASET_TARGET,\n#                            verbose=True)\n\n# print(\"\\nStarting data generation and sampling...\")\n# generated_data_info = data_gen.generate() # This will now save individual files\n\n# print(\"\\n--- Data Generation and Sampling Complete ---\")\n# for name, num_samples in generated_data_info.items():\n#     print(f\"Dataset: {name}, Samples saved: {num_samples}\")\n\n# print(\"\\nGenerated files in:\", GENERATED_DATA_DIR)\n# for f_name in os.listdir(GENERATED_DATA_DIR):\n#     if f_name.endswith(\"_20k.jsonl\"):\n#         print(os.path.join(GENERATED_DATA_DIR, f_name))\n#         # Quick check of one file\n#         if f_name == \"mnli_20k.jsonl\": # Example check\n#             print(f\"  First line of {f_name}:\")\n#             with open(os.path.join(GENERATED_DATA_DIR, f_name), 'r') as f_check:\n#                 print(f\"    {f_check.readline().strip()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T23:13:57.557848Z","iopub.execute_input":"2025-05-28T23:13:57.558823Z","iopub.status.idle":"2025-05-28T23:13:57.610939Z","shell.execute_reply.started":"2025-05-28T23:13:57.558787Z","shell.execute_reply":"2025-05-28T23:13:57.609898Z"}},"outputs":[{"name":"stdout","text":"Initializing DataGenerator for: ['mnli', 'paws', 'snli', 'anli_r1', 'anli_r2', 'anli_r3', 'mrpc', 'qqp', 'doc_nli', 'stsb', 'sick', 'vitaminc', 'nli_fever', 'xsum', 'cnndm', 'boolq']\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3964384766.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Initializing DataGenerator for: {datasets_to_generate}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Set verbose=False for less tqdm output during actual runs if preferred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m data_gen = DataGenerator(datasets_to_generate,\n\u001b[0m\u001b[1;32m     40\u001b[0m                            \u001b[0mtarget_samples_per_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSAMPLES_PER_DATASET_TARGET\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                            verbose=True)\n","\u001b[0;31mNameError\u001b[0m: name 'DataGenerator' is not defined"],"ename":"NameError","evalue":"name 'DataGenerator' is not defined","output_type":"error"}],"execution_count":24},{"cell_type":"markdown","source":"# Dataloader Definitions","metadata":{}},{"cell_type":"code","source":"from typing import Optional, Tuple, List\n\nclass DSTDataSet(Dataset):\n    def __init__(self, dataset_paths_with_task: list, model_name='roberta-large', need_mlm=True, tokenizer_max_length=512):\n        super().__init__()\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.tokenizer_max_length = tokenizer_max_length\n        self.need_mlm = need_mlm\n        \n        self.dataset = []\n        print(f\"Loading data for DSTDataSet...\")\n        for item in tqdm(dataset_paths_with_task, desc=\"Loading individual JSONL files\"):\n            path = item['path']\n            task_type_from_path = item['task_type'] # Get task_type passed along with path\n            try:\n                with open(path, 'r', encoding='utf8') as f:\n                    for line_idx, line in enumerate(f):\n                        try:\n                            loaded_json = json.loads(line)\n                            # Ensure the loaded JSON has the 'task' field, if not, use the one from path config\n                            if 'task' not in loaded_json:\n                                loaded_json['task'] = task_type_from_path\n                            self.dataset.append(loaded_json)\n                        except json.JSONDecodeError:\n                            print(f\"Warning: Could not decode JSON from line {line_idx} in {path}. Skipping: {line.strip()}\")\n            except FileNotFoundError:\n                 print(f\"Warning: File not found {path}. Skipping.\")\n        \n        if not self.dataset:\n            print(\"Warning: DSTDataSet loaded 0 examples. Check paths and file contents.\")\n        else:\n            print(f\"DSTDataSet loaded a total of {len(self.dataset)} examples from {len(dataset_paths_with_task)} task files.\")\n\n\n    def random_word(self, token_ids_list):\n        tokens = list(token_ids_list) \n        if not self.need_mlm:\n            return tokens, [-100] * len(tokens)\n\n        output_label = []\n        for i, token_id in enumerate(tokens):\n            if token_id == self.tokenizer.pad_token_id:\n                output_label.append(-100)\n                continue\n            prob = random.random()\n            if prob < 0.15:\n                prob /= 0.15\n                original_token_id = tokens[i]\n\n                if prob < 0.8: \n                    tokens[i] = self.tokenizer.mask_token_id\n                elif prob < 0.9: \n                    tokens[i] = random.choice(list(range(self.tokenizer.vocab_size)))\n                output_label.append(original_token_id) \n            else:\n                output_label.append(-100)\n        return tokens, output_label\n\n    def _process_common(self, index):\n        example = self.dataset[index]\n        text_a = str(example['text_a']) # Ensure string\n        \n        # Positive example from text_b\n        if example['text_b']:\n            text_b_candidate = str(random.choice(example['text_b'])) # Ensure string\n            current_label = 1 # Default assumption for text_b\n        else: # Should not happen if data gen is correct\n            text_b_candidate = \"\"\n            current_label = 0 \n\n        # Negative example from text_c (if available and 50% chance)\n        # This logic is simplified. The original AlignScore had more nuanced positive/negative pair creation.\n        # The pre-processed data from DataGenerator already implies the roles of text_b (pos) and text_c (neg hints for some tasks)\n        # The 'orig_label' from the JSONL is the primary source of truth for labels.\n        \n        final_text_b = text_b_candidate # For this iteration, we primarily use text_b\n                                    # and derive labels from 'orig_label'.\n                                    # If orig_label is -1 (e.g. summarization), special handling might be needed\n                                    # for pos/neg pairs, but DSTDataLoader expects single pairs.\n                                    # The original AlignScore created multiple pairs in each process_ function.\n                                    # Here, each line in JSONL is one pair.\n\n        try:\n            tokenized_pair = self.tokenizer(text_a, final_text_b, padding='max_length', \n                                            max_length=self.tokenizer_max_length, truncation='only_first')\n        except: \n            tokenized_pair = self.tokenizer(text_a, final_text_b, padding='max_length', \n                                            max_length=self.tokenizer_max_length, truncation=True)\n\n        input_ids, mlm_labels = self.random_word(tokenized_pair['input_ids'])\n        \n        orig_label = example['orig_label']\n        task_type = example['task']\n\n        align_label_val = -100\n        tri_label_val = -100\n        reg_label_val = -100.0\n\n        if task_type in ['nli', 'fact_checking']:\n            tri_label_val = int(orig_label) if orig_label in [0,1,2] else 1 \n        elif task_type in ['paraphrase', 'bin_nli', 'qa', 'coreference', \n                           'summarization', 'ir', 'multiple_choice_qa', 'extractive_qa']:\n             # For summarization with orig_label = -1, this will become 0.\n             # Training logic needs to handle how summarization pairs are made (e.g. from text_b vs text_c)\n             # For now, if orig_label is -1 (from sum), assume it's a positive pair (text_a, text_b[0])\n             if orig_label == -1 and task_type == 'summarization': # Positive pair from text_b\n                 align_label_val = 1\n             elif orig_label in [0,1]: # For other binary tasks\n                 align_label_val = int(orig_label)\n             else: # Default to 0 if label is unexpected\n                 align_label_val = 0\n        elif task_type in ['sts', 'wmt', 'ctc']:\n            try:\n                reg_label_val = float(orig_label)\n            except ValueError:\n                print(f\"Warning: Could not convert reg_label '{orig_label}' to float for task {task_type}. Using 0.0.\")\n                reg_label_val = 0.0\n\n\n        return (\n            torch.tensor(input_ids),\n            torch.tensor(tokenized_pair['attention_mask']),\n            torch.tensor(tokenized_pair.get('token_type_ids', [0]*len(input_ids))), \n            torch.tensor(align_label_val, dtype=torch.long), # Ensure long for CrossEntropy\n            torch.tensor(mlm_labels, dtype=torch.long),\n            torch.tensor(tri_label_val, dtype=torch.long),\n            torch.tensor(reg_label_val, dtype=torch.float) # Ensure float\n        )\n\n    def __getitem__(self, index):\n        input_ids, attention_mask, token_type_ids, align_label, mlm_labels, tri_label, reg_label = self._process_common(index)\n        \n        item = {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'align_label': align_label,\n            'mlm_label': mlm_labels,\n            'tri_label': tri_label,\n            'reg_label': reg_label \n        }\n        # Only add token_type_ids if they are not all zeros (i.e., actually used, like for BERT)\n        # RoBERTa doesn't use them, tokenizer might return zeros.\n        if token_type_ids is not None and not torch.all(token_type_ids == 0):\n             item['token_type_ids'] = token_type_ids\n        return item\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nclass DSTDataLoader(LightningDataModule):\n    def __init__(self, train_files_config_list: list, val_files_config_list: list, \n                 model_name='roberta-large', need_mlm=True, tokenizer_max_length=512, \n                 train_batch_size=32, eval_batch_size=16, num_workers=2, **kwargs):\n        super().__init__(**kwargs)\n        # train_files_config_list is like [{'path': 'path/to/mnli_20k.jsonl', 'task_type': 'nli'}, ...]\n        self.train_files_config_list = train_files_config_list\n        self.val_files_config_list = val_files_config_list    \n        self.model_name = model_name\n        self.need_mlm = need_mlm\n        self.tokenizer_max_length = tokenizer_max_length\n        self.train_batch_size = train_batch_size\n        self.eval_batch_size = eval_batch_size\n        self.num_workers = num_workers\n\n        self.train_dataset = None\n        self.val_dataset = None\n\n    def setup(self, stage: Optional[str] = None) -> None:\n        if stage == 'fit' or stage is None:\n            if not self.train_dataset:\n                print(\"Setting up training data for DataLoader...\")\n                self.train_dataset = DSTDataSet(dataset_paths_with_task=self.train_files_config_list, \n                                                model_name=self.model_name, need_mlm=self.need_mlm, \n                                                tokenizer_max_length=self.tokenizer_max_length)\n            if not self.val_dataset:\n                print(\"Setting up validation data for DataLoader...\")\n                self.val_dataset = DSTDataSet(dataset_paths_with_task=self.val_files_config_list, \n                                              model_name=self.model_name, need_mlm=self.need_mlm, \n                                              tokenizer_max_length=self.tokenizer_max_length)\n                                              \n    def train_dataloader(self):\n        if not self.train_dataset: self.setup('fit')\n        return DataLoader(self.train_dataset, batch_size=self.train_batch_size, shuffle=True, \n                          num_workers=self.num_workers, pin_memory=True, persistent_workers= (self.num_workers > 0) )\n\n    def val_dataloader(self):\n        if not self.val_dataset: self.setup('fit')\n        return DataLoader(self.val_dataset, batch_size=self.eval_batch_size, shuffle=False, \n                          num_workers=self.num_workers, pin_memory=True, persistent_workers= (self.num_workers > 0) )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T23:13:37.292933Z","iopub.execute_input":"2025-05-28T23:13:37.293469Z","iopub.status.idle":"2025-05-28T23:13:37.312302Z","shell.execute_reply.started":"2025-05-28T23:13:37.293445Z","shell.execute_reply":"2025-05-28T23:13:37.311482Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# Model Definitions\n","metadata":{}},{"cell_type":"code","source":"# Cell 5: Model Definition (Adapted for PyTorch Lightning v2.0.0+)\n\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, List \n\n# --- ModelOutput Dataclass --- \n@dataclass\nclass ModelOutput:\n    loss: Optional[torch.FloatTensor] = None\n    all_loss: Optional[list] = None\n    loss_nums: Optional[list] = None\n    prediction_logits: torch.FloatTensor = None\n    seq_relationship_logits: torch.FloatTensor = None\n    tri_label_logits: torch.FloatTensor = None\n    reg_label_logits: torch.FloatTensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None\n\n# --- BERTAlignModel Class (Updated for PTL v2.0.0+) ---\nclass BERTAlignModel(pl.LightningModule):\n    def __init__(self, model_name_or_path='roberta-large', using_pretrained=True,\n                 learning_rate=1e-5, adam_epsilon=1e-6, weight_decay=0.1,\n                 warmup_steps_portion=0.06, mlm_loss_factor=0.5, **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.model_name_or_path = model_name_or_path\n        self.using_pretrained = using_pretrained\n        \n        config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)\n\n        # Model loading logic (simplified for brevity, use your existing comprehensive one)\n        if 'roberta' in model_name_or_path.lower():\n            if using_pretrained:\n                self.base_model = RobertaModel.from_pretrained(model_name_or_path, config=config, trust_remote_code=True)\n                lm_head_model = RobertaForMaskedLM.from_pretrained(model_name_or_path, config=config, trust_remote_code=True)\n                self.mlm_head = lm_head_model.lm_head\n            else:\n                self.base_model = RobertaModel(config)\n                self.mlm_head = RobertaForMaskedLM(config).lm_head\n        elif 'bert' in model_name_or_path.lower() and 'electra' not in model_name_or_path.lower():\n            if using_pretrained:\n                bert_mlm_model = BertForPreTraining.from_pretrained(model_name_or_path, config=config, trust_remote_code=True)\n                self.base_model = bert_mlm_model.bert\n                self.mlm_head = bert_mlm_model.cls.predictions\n            else:\n                self.base_model = BertModel(config)\n                self.mlm_head = BertForPreTraining(config).cls.predictions\n        else:\n            print(f\"Warning: Model type {model_name_or_path} needs specific loading logic for base_model/mlm_head if not roberta/bert.\")\n            self.base_model = AutoModel.from_pretrained(model_name_or_path, config=config, trust_remote_code=True)\n            self.mlm_head = AutoModelForMaskedLM.from_pretrained(model_name_or_path, config=config, trust_remote_code=True).get_output_embeddings() # Or other appropriate head\n\n        self.hidden_size = config.hidden_size\n        self.bin_layer = nn.Linear(self.hidden_size, 2)\n        self.tri_layer = nn.Linear(self.hidden_size, 3)\n        self.reg_layer = nn.Linear(self.hidden_size, 1)\n        self.dropout = nn.Dropout(p=config.hidden_dropout_prob if hasattr(config, 'hidden_dropout_prob') else 0.1)\n        \n        self.need_mlm = True \n        \n        # For storing step outputs\n        self._train_step_outputs = []\n        self._validation_step_outputs = []\n\n\n    def forward(self, batch):\n        # (Your existing forward pass logic - ensure it returns ModelOutput correctly)\n        # ... (Copied from your previous BERTAlignModel.forward())\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        token_type_ids = batch.get('token_type_ids', None)\n        \n        if 'roberta' in self.hparams.model_name_or_path.lower() and token_type_ids is not None:\n            base_model_output = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        else:\n            base_model_output = self.base_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        \n        sequence_output = base_model_output.last_hidden_state\n        if hasattr(base_model_output, 'pooler_output') and base_model_output.pooler_output is not None:\n            pooler_output = base_model_output.pooler_output\n        else: \n            pooler_output = sequence_output[:, 0, :]\n\n        prediction_scores = self.mlm_head(sequence_output) \n        pooled_output_dropout = self.dropout(pooler_output)\n        seq_relationship_score = self.bin_layer(pooled_output_dropout)\n        tri_label_score = self.tri_layer(pooled_output_dropout)\n        reg_label_score = self.reg_layer(pooler_output)\n\n        all_losses_val = [torch.tensor(0.0, device=self.device)] * 4\n        loss_nums_val = [torch.tensor(0, device=self.device, dtype=torch.long)] * 4\n\n        if 'mlm_label' in batch:\n            ce_loss_fct_sum = nn.CrossEntropyLoss(reduction='sum', ignore_index=-100)\n            if self.need_mlm:\n                mlm_labels = batch['mlm_label'].view(-1)\n                masked_lm_loss = ce_loss_fct_sum(prediction_scores.view(-1, self.base_model.config.vocab_size), mlm_labels)\n                loss_nums_val[0] = torch.sum(mlm_labels != -100)\n                all_losses_val[0] = masked_lm_loss if loss_nums_val[0] > 0 else torch.tensor(0.0, device=self.device)\n\n            align_labels = batch['align_label'].view(-1)\n            valid_align_indices = align_labels != -100\n            if torch.any(valid_align_indices):\n                next_sentence_loss = ce_loss_fct_sum(seq_relationship_score.view(-1, 2)[valid_align_indices], align_labels[valid_align_indices])\n                all_losses_val[1] = next_sentence_loss / math.log(2) if math.log(2) > 0 else next_sentence_loss\n                loss_nums_val[1] = torch.sum(valid_align_indices)\n            \n            tri_labels = batch['tri_label'].view(-1)\n            valid_tri_indices = tri_labels != -100\n            if torch.any(valid_tri_indices):\n                tri_label_loss = ce_loss_fct_sum(tri_label_score.view(-1, 3)[valid_tri_indices], tri_labels[valid_tri_indices])\n                all_losses_val[2] = tri_label_loss / math.log(3) if math.log(3) > 0 else tri_label_loss\n                loss_nums_val[2] = torch.sum(valid_tri_indices)\n\n            reg_labels = batch['reg_label'].view(-1) \n            valid_reg_indices = reg_labels != -100.0\n            if torch.any(valid_reg_indices):\n                current_reg_preds = reg_label_score.view(-1)[valid_reg_indices]\n                current_reg_targets = reg_labels[valid_reg_indices]\n                reg_label_loss = self.mse_loss(current_reg_preds, current_reg_targets, reduction='sum')\n                loss_nums_val[3] = torch.sum(valid_reg_indices)\n                all_losses_val[3] = reg_label_loss\n        \n        return ModelOutput(\n            all_loss=all_losses_val, loss_nums=loss_nums_val, \n            prediction_logits=prediction_scores, seq_relationship_logits=seq_relationship_score, \n            tri_label_logits=tri_label_score, reg_label_logits=reg_label_score, \n            hidden_states=base_model_output.hidden_states if hasattr(base_model_output, 'hidden_states') else None,\n            attentions=base_model_output.attentions if hasattr(base_model_output, 'attentions') else None\n        )\n\n\n    def _calculate_and_log_epoch_end_metrics(self, step_outputs: List[dict], step_type=\"train\"):\n        # Common logic for aggregating and logging epoch-end metrics\n        agg_losses = [0.0] * 4\n        agg_loss_nums = [0] * 4\n\n        for step_output in step_outputs: # step_outputs is the list of dicts we stored\n            losses_sum_batch = step_output['losses_sum_batch'] # List of loss sums for the batch\n            loss_nums_batch = step_output['loss_nums_batch'] # List of item counts for each loss for the batch\n            for i in range(4):\n                agg_losses[i] += losses_sum_batch[i].item()\n                agg_loss_nums[i] += loss_nums_batch[i].item()\n\n        avg_mlm_loss = agg_losses[0] / agg_loss_nums[0] if agg_loss_nums[0] > 0 else 0.\n        avg_bin_loss = agg_losses[1] / agg_loss_nums[1] if agg_loss_nums[1] > 0 else 0.\n        avg_tri_loss = agg_losses[2] / agg_loss_nums[2] if agg_loss_nums[2] > 0 else 0.\n        avg_reg_loss = agg_losses[3] / agg_loss_nums[3] if agg_loss_nums[3] > 0 else 0.\n        \n        mlm_contribution = self.hparams.mlm_loss_factor * avg_mlm_loss if self.need_mlm else 0.\n        # Ensure all components are non-None or zero before summing\n        total_loss = mlm_contribution + avg_bin_loss + avg_tri_loss + avg_reg_loss\n        \n        self.log(f'{step_type}_loss_epoch', total_loss, prog_bar=True, sync_dist=True)\n        if self.need_mlm: self.log(f'{step_type}_mlm_loss_epoch', avg_mlm_loss, prog_bar=False, sync_dist=True)\n        self.log(f'{step_type}_bin_loss_epoch', avg_bin_loss, prog_bar=False, sync_dist=True)\n        self.log(f'{step_type}_tri_loss_epoch', avg_tri_loss, prog_bar=False, sync_dist=True)\n        self.log(f'{step_type}_reg_loss_epoch', avg_reg_loss, prog_bar=False, sync_dist=True)\n        \n        if step_type == \"val\": # For checkpointing\n            self.log(\"val_loss\", total_loss, prog_bar=True, sync_dist=True) # PTL uses 'val_loss'\n\n        return total_loss\n\n\n    def training_step(self, batch, batch_idx):\n        output = self(batch)\n        # Calculate per-item average loss for this batch for PTL optimizer\n        current_total_loss = torch.tensor(0.0, device=self.device)\n        items_for_loss = 0\n\n        if output.loss_nums[0] > 0 and self.need_mlm:\n            current_total_loss += self.hparams.mlm_loss_factor * output.all_loss[0] # loss is already sum\n            items_for_loss += output.loss_nums[0] # accumulate items for averaging later if needed\n        if output.loss_nums[1] > 0: \n            current_total_loss += output.all_loss[1]\n            items_for_loss += output.loss_nums[1]\n        if output.loss_nums[2] > 0: \n            current_total_loss += output.all_loss[2]\n            items_for_loss += output.loss_nums[2]\n        if output.loss_nums[3] > 0: \n            current_total_loss += output.all_loss[3]\n            items_for_loss += output.loss_nums[3]\n        \n        # Normalize the sum of losses by total items contributing to any loss for the step loss\n        # PTL backward() will be called on this returned 'loss'\n        step_loss = current_total_loss / items_for_loss if items_for_loss > 0 else torch.tensor(0.0, device=self.device)\n\n        self.log('train_step_loss', step_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True)\n        \n        # For epoch end calculation, store the sums and counts\n        self._train_step_outputs.append({'losses_sum_batch': output.all_loss, 'loss_nums_batch': output.loss_nums})\n        return step_loss # Return the scalar loss for PTL to optimize\n\n    def on_train_epoch_end(self):\n        if not self._train_step_outputs: return\n        self._calculate_and_log_epoch_end_metrics(self._train_step_outputs, step_type=\"train\")\n        self._train_step_outputs.clear()  # free memory\n\n\n    def validation_step(self, batch, batch_idx):\n        output = self(batch)\n        # For epoch end calculation, store the sums and counts\n        self._validation_step_outputs.append({'losses_sum_batch': output.all_loss, 'loss_nums_batch': output.loss_nums})\n        # No need to return loss here, PTL will call on_validation_epoch_end\n\n    def on_validation_epoch_end(self):\n        if not self._validation_step_outputs: return\n        self._calculate_and_log_epoch_end_metrics(self._validation_step_outputs, step_type=\"val\")\n        self._validation_step_outputs.clear()  # free memory\n\n    def configure_optimizers(self):\n        # (Your existing configure_optimizers logic - should be fine)\n        # ... (Copied from your previous BERTAlignModel.configure_optimizers())\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad],\n                \"weight_decay\": self.hparams.weight_decay,\n            },\n            {\n                \"params\": [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n\n        # Calculate total steps based on trainer if available, otherwise estimate\n        if self.trainer and hasattr(self.trainer, 'estimated_stepping_batches') and self.trainer.estimated_stepping_batches is not None and self.trainer.estimated_stepping_batches > 0 :\n            num_training_steps = self.trainer.estimated_stepping_batches\n            print(f\"Scheduler: Using trainer.estimated_stepping_batches = {num_training_steps}\")\n        else:\n            # Estimate total steps if trainer attribute not available (e.g. before trainer.fit() fully initializes it)\n            # This estimation requires dataloader length.\n            # For robust setup, this might be better configured in configure_sharded_model or after dataloaders are known\n            # Let's try to access dataloader directly if trainer property isn't ready.\n            if self.trainer and self.trainer.train_dataloader:\n                 # This might be tricky as train_dataloader itself might not be fully initialized yet by PTL at this exact point\n                 # Let's assume PTL's internal handling or a large default if this fails.\n                try:\n                    len_train_loader = len(self.trainer.train_dataloader)\n                    accumulate_grad_batches = self.trainer.accumulate_grad_batches if self.trainer.accumulate_grad_batches else 1\n                    num_training_steps = (len_train_loader // accumulate_grad_batches) * self.trainer.max_epochs\n                    print(f\"Scheduler: Estimated num_training_steps = {num_training_steps}\")\n                except Exception as e:\n                    print(f\"Scheduler: Could not estimate steps from dataloader ({e}). Using large default for num_training_steps.\")\n                    num_training_steps = 100000 # Fallback\n            else:\n                 print(f\"Scheduler: Trainer or train_dataloader not available for step estimation. Using large default.\")\n                 num_training_steps = 100000 # Fallback if trainer/dataloader info not yet available\n            \n            if num_training_steps <= 0 : # Safety for a very small dataset/epochs\n                 print(f\"Warning: Calculated num_training_steps is {num_training_steps}. Setting to a minimum of 100.\")\n                 num_training_steps = 100\n\n\n        num_warmup_steps = int(self.hparams.warmup_steps_portion * num_training_steps)\n        print(f\"Scheduler: num_warmup_steps = {num_warmup_steps}, num_training_steps = {num_training_steps}\")\n        \n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps,\n        )\n        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}}\n\n    def mse_loss(self, input_tensor, target_tensor, reduction='mean'):\n        target_tensor_float = target_tensor.float()\n        if input_tensor.numel() == 0 or target_tensor_float.numel() == 0 or input_tensor.shape != target_tensor_float.shape :\n             # print(f\"MSE Loss: Mismatch or empty tensors. Input: {input_tensor.shape}, Target: {target_tensor_float.shape}\")\n             return torch.tensor(0.0, device=self.device)\n\n        loss = nn.functional.mse_loss(input_tensor, target_tensor_float, reduction='none')\n        \n        if reduction == \"mean\":\n            return loss.mean() if loss.numel() > 0 else torch.tensor(0.0, device=self.device)\n        elif reduction == \"sum\":\n            return loss.sum()\n        return torch.tensor(0.0, device=self.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T23:07:54.029805Z","iopub.execute_input":"2025-05-28T23:07:54.030347Z","iopub.status.idle":"2025-05-28T23:07:54.064757Z","shell.execute_reply.started":"2025-05-28T23:07:54.030324Z","shell.execute_reply":"2025-05-28T23:07:54.063934Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Training Execution","metadata":{}},{"cell_type":"code","source":"# # --- Training Configuration ---\n# TRAIN_ARGS = Namespace(\n#     model_name_or_path=MODEL_NAME, \n#     batch_size=4, # Adjust based on Kaggle GPU VRAM (RoBERTa-large is demanding)\n#     num_epoch=3, # Start with 1 for Kaggle\n#     num_workers=2, \n#     learning_rate=1e-5, \n#     adam_epsilon=1e-6, \n#     weight_decay=0.01, # Common default for AdamW\n#     warmup_steps_portion=0.06, \n#     ckpt_save_path=CHECKPOINT_SAVE_PATH,\n#     ckpt_comment=\"kaggle_20k_alignscore_\", \n#     devices=1 if torch.cuda.is_available() else 0, \n#     accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n#     accumulate_grad_batches=8, # Effective batch size = batch_size * accumulate_grad_batches (4*8=32)\n#     do_mlm=True, \n#     use_pretrained_model=True,\n#     mlm_loss_factor=0.5 # From BERTAlignModel default\n# )\n\n# # --- Prepare dataset config for DataLoader ---\n# # Uses files generated in GENERATED_DATA_DIR (e.g., mnli_20k.jsonl)\n# # Create list of {'path': ..., 'task_type': ...} dicts\n# train_files_config_list = []\n# val_files_config_list = []\n\n# # all_generated_files = [f for f in os.listdir(GENERATED_DATA_DIR) if f.endswith(\"_20k.jsonl\")]\n# # all_generated_files = [f for f in os.listdir(\"/kaggle/input/samples-fact-checking-adjacent-datasets\") if f.endswith(\"_20k.jsonl\")]\n# all_generated_files = [f for f in os.listdir(\"/kaggle/input/nli-datasets-alignscore\") if f.endswith(\"_20k.jsonl\")]\n\n# # KAGGLE_INPUT_DIR = \"/kaggle/input/samples-fact-checking-adjacent-datasets\"\n\n\n# random.shuffle(all_generated_files)\n\n# # Simple split of generated files for train/val\n# # For a more robust setup, ensure diverse tasks in validation or use predefined splits\n# num_val_files = max(1, int(0.1 * len(all_generated_files))) # Use at least 1 file for validation\n# if not all_generated_files:\n#     print(\"No generated JSONL files found in {GENERATED_DATA_DIR}. Cannot proceed with training.\")\n#     # Handle error or exit\n# else:\n#     for i, f_name in enumerate(all_generated_files):\n#         dataset_name_key = f_name.replace(\"_20k.jsonl\", \"\")\n#         task_type = DATASET_CONFIG.get(dataset_name_key, {}).get('task', 'unknown') # Get task from main config\n#         file_info = {'path': os.path.join(GENERATED_DATA_DIR, f_name), 'task_type': task_type}\n#         if i < num_val_files and len(all_generated_files) > 1 : # Ensure train set is not empty\n#             val_files_config_list.append(file_info)\n#         else:\n#             train_files_config_list.append(file_info)\n            \n#     if not train_files_config_list and val_files_config_list: # If all went to val somehow (e.g. only 1 file)\n#         train_files_config_list = val_files_config_list \n#         print(\"Warning: Using the same file(s) for training and validation as only a few files were generated.\")\n#     elif not val_files_config_list and train_files_config_list: # If no val files, use one from train\n#         val_files_config_list.append(train_files_config_list[0])\n#         print(f\"Warning: No dedicated validation files. Using {train_files_config_list[0]['path']} for validation.\")\n\n\n# if not train_files_config_list:\n#     print(\"No training data files configured. Skipping training.\")\n#     BEST_MODEL_PATH = None\n# else:\n#     print(f\"Training with {len(train_files_config_list)} task files. Validation with {len(val_files_config_list)} task files.\")\n#     print(\"Training files:\", [item['path'] for item in train_files_config_list[:3]], \"...\")\n#     print(\"Validation files:\", [item['path'] for item in val_files_config_list[:3]], \"...\")\n\n#     data_module = DSTDataLoader(\n#         train_files_config_list=train_files_config_list,\n#         val_files_config_list=val_files_config_list, \n#         model_name=TRAIN_ARGS.model_name_or_path,\n#         train_batch_size=TRAIN_ARGS.batch_size,\n#         eval_batch_size=TRAIN_ARGS.batch_size * 2, \n#         num_workers=TRAIN_ARGS.num_workers,\n#         need_mlm=TRAIN_ARGS.do_mlm\n#     )\n\n#     align_model = BERTAlignModel(\n#         model_name_or_path=TRAIN_ARGS.model_name_or_path,\n#         using_pretrained=TRAIN_ARGS.use_pretrained_model,\n#         adam_epsilon=TRAIN_ARGS.adam_epsilon,\n#         learning_rate=TRAIN_ARGS.learning_rate,\n#         weight_decay=TRAIN_ARGS.weight_decay,\n#         warmup_steps_portion=TRAIN_ARGS.warmup_steps_portion, # Now this will match\n#         mlm_loss_factor=TRAIN_ARGS.mlm_loss_factor\n#     )\n#     align_model.need_mlm = TRAIN_ARGS.do_mlm\n\n#     model_short_name = TRAIN_ARGS.model_name_or_path.split('/')[-1]\n#     checkpoint_filename_template = (\n#         f\"{TRAIN_ARGS.ckpt_comment}{model_short_name}_\"\n#         f\"{'scratch_' if not TRAIN_ARGS.use_pretrained_model else ''}\"\n#         f\"{'mlm_' if TRAIN_ARGS.do_mlm else 'no_mlm_'}\"\n#         f\"all_{SAMPLES_PER_DATASET_TARGET // 1000}k_\"\n#         f\"b{TRAIN_ARGS.batch_size}x{TRAIN_ARGS.devices}x{TRAIN_ARGS.accumulate_grad_batches}\"\n#     )\n\n#     checkpoint_callback = pl.callbacks.ModelCheckpoint(\n#         dirpath=TRAIN_ARGS.ckpt_save_path,\n#         filename=checkpoint_filename_template + \"_{epoch:02d}_{val_loss:.2f}\",\n#         save_top_k=1,\n#         monitor=\"val_loss\", # Monitor val_loss (which is total weighted loss on val set)\n#         mode=\"min\",\n#         every_n_epochs=1,\n#         save_last=True # Save the last model as well\n#     )\n    \n#     # Early stopping\n#     early_stop_callback = pl.callbacks.EarlyStopping(\n#         monitor=\"val_loss\",\n#         min_delta=0.001,\n#         patience=3, # Stop if val_loss doesn't improve for 3 epochs\n#         verbose=True,\n#         mode=\"min\"\n#     )\n\n#     trainer = pl.Trainer(\n#         accelerator=TRAIN_ARGS.accelerator,\n#         devices=TRAIN_ARGS.devices if TRAIN_ARGS.accelerator == 'gpu' else None,\n#         max_epochs=TRAIN_ARGS.num_epoch,\n#         callbacks=[checkpoint_callback, early_stop_callback],\n#         accumulate_grad_batches=TRAIN_ARGS.accumulate_grad_batches,\n#         precision=\"16-mixed\" if TRAIN_ARGS.accelerator == 'gpu' else 32, # Mixed precision for GPU\n#         log_every_n_steps=20, # Log more frequently\n#         # deterministic=True, # For reproducibility, can slow down\n#         # strategy=\"ddp_find_unused_parameters_true\" if TRAIN_ARGS.devices > 1 else None, # If using multiple GPUs\n#     )\n\n#     print(f\"\\n--- Starting Training: {checkpoint_filename_template} ---\")\n#     try:\n#         trainer.fit(align_model, datamodule=data_module)\n        \n#         # Path to the best checkpoint saved by ModelCheckpoint\n#         BEST_MODEL_PATH = checkpoint_callback.best_model_path\n#         print(f\"Best checkpoint path from callback: {BEST_MODEL_PATH}\")\n#         if not BEST_MODEL_PATH or not os.path.exists(BEST_MODEL_PATH):\n#             print(\"Best model path not found from callback, trying 'last.ckpt'\")\n#             # Try last checkpoint if best_model_path is not set (e.g. if training is very short or no val improvement)\n#             last_ckpt_path = os.path.join(TRAIN_ARGS.ckpt_save_path, \"last.ckpt\")\n#             if os.path.exists(last_ckpt_path):\n#                  BEST_MODEL_PATH = last_ckpt_path\n#             else: # Fallback to a constructed name if last.ckpt also not there\n#                  BEST_MODEL_PATH = os.path.join(TRAIN_ARGS.ckpt_save_path, f\"{checkpoint_filename_template}_final.ckpt\") # Default if no val improvement\n#                  trainer.save_checkpoint(BEST_MODEL_PATH)\n\n\n#         print(f\"Training finished. Best/Final model saved to: {BEST_MODEL_PATH}\")\n\n#     except Exception as e:\n#         print(f\"An error occurred during training: {e}\")\n#         import traceback\n#         traceback.print_exc()\n#         BEST_MODEL_PATH = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T23:24:22.175869Z","iopub.execute_input":"2025-05-28T23:24:22.176686Z","iopub.status.idle":"2025-05-28T23:24:25.500358Z","shell.execute_reply.started":"2025-05-28T23:24:22.176649Z","shell.execute_reply":"2025-05-28T23:24:25.499548Z"}},"outputs":[{"name":"stdout","text":"Training with 6 task files. Validation with 1 task files.\nTraining files: ['/kaggle/working/alignscore_training_data_20k/mnli_20k.jsonl', '/kaggle/working/alignscore_training_data_20k/anli_r2_20k.jsonl', '/kaggle/working/alignscore_training_data_20k/snli_20k.jsonl'] ...\nValidation files: ['/kaggle/working/alignscore_training_data_20k/doc_nli_20k.jsonl'] ...\n\n--- Starting Training: kaggle_20k_alignscore_roberta-large_mlm_all_20k_b4x1x8 ---\nSetting up training data for DataLoader...\nLoading data for DSTDataSet...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading individual JSONL files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bafd2510b0448678f5e018da7f9d753"}},"metadata":{}},{"name":"stdout","text":"Warning: File not found /kaggle/working/alignscore_training_data_20k/mnli_20k.jsonl. Skipping.\nWarning: File not found /kaggle/working/alignscore_training_data_20k/anli_r2_20k.jsonl. Skipping.\nWarning: File not found /kaggle/working/alignscore_training_data_20k/snli_20k.jsonl. Skipping.\nWarning: File not found /kaggle/working/alignscore_training_data_20k/anli_r1_20k.jsonl. Skipping.\nWarning: File not found /kaggle/working/alignscore_training_data_20k/nli_fever_20k.jsonl. Skipping.\nWarning: File not found /kaggle/working/alignscore_training_data_20k/anli_r3_20k.jsonl. Skipping.\nWarning: DSTDataSet loaded 0 examples. Check paths and file contents.\nSetting up validation data for DataLoader...\nLoading data for DSTDataSet...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading individual JSONL files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35f7a0279f504cd49b50fd551ae35211"}},"metadata":{}},{"name":"stdout","text":"Warning: File not found /kaggle/working/alignscore_training_data_20k/doc_nli_20k.jsonl. Skipping.\nWarning: DSTDataSet loaded 0 examples. Check paths and file contents.\nSetting up training data for DataLoader...\nLoading data for DSTDataSet...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading individual JSONL files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f68569639b67459abe791c7168f78d7e"}},"metadata":{}},{"name":"stdout","text":"Warning: File not found /kaggle/working/alignscore_training_data_20k/mnli_20k.jsonl. Skipping.\nWarning: File not found /kaggle/working/alignscore_training_data_20k/anli_r2_20k.jsonl. Skipping.\nWarning: File not found /kaggle/working/alignscore_training_data_20k/snli_20k.jsonl. Skipping.\nWarning: File not found /kaggle/working/alignscore_training_data_20k/anli_r1_20k.jsonl. Skipping.\nWarning: File not found /kaggle/working/alignscore_training_data_20k/nli_fever_20k.jsonl. Skipping.\nWarning: File not found /kaggle/working/alignscore_training_data_20k/anli_r3_20k.jsonl. Skipping.\nWarning: DSTDataSet loaded 0 examples. Check paths and file contents.\nSetting up validation data for DataLoader...\nLoading data for DSTDataSet...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading individual JSONL files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9a9c9ac7a1d40bbabeeb9e8738c194e"}},"metadata":{}},{"name":"stdout","text":"Warning: File not found /kaggle/working/alignscore_training_data_20k/doc_nli_20k.jsonl. Skipping.\nWarning: DSTDataSet loaded 0 examples. Check paths and file contents.\nAn error occurred during training: num_samples should be a positive integer value, but got num_samples=0\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_35/1405545594.py\", line 131, in <cell line: 0>\n    trainer.fit(align_model, datamodule=data_module)\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n    call._call_and_handle_interrupt(\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n    return trainer_fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 988, in _run\n    self.strategy.setup(self)\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\", line 159, in setup\n    self.setup_optimizers(trainer)\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\", line 139, in setup_optimizers\n    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)\n                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/optimizer.py\", line 180, in _init_optimizers_and_lr_schedulers\n    optim_conf = call._call_lightning_module_hook(model.trainer, \"configure_optimizers\", pl_module=model)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_35/2246031482.py\", line 230, in configure_optimizers\n    if self.trainer and hasattr(self.trainer, 'estimated_stepping_batches') and self.trainer.estimated_stepping_batches is not None and self.trainer.estimated_stepping_batches > 0 :\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1707, in estimated_stepping_batches\n    self.fit_loop.setup_data()\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 237, in setup_data\n    train_dataloader = _request_dataloader(source)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py\", line 326, in _request_dataloader\n    return data_source.dataloader()\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py\", line 293, in dataloader\n    return call._call_lightning_datamodule_hook(self.instance.trainer, self.name)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\", line 198, in _call_lightning_datamodule_hook\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_35/2184769980.py\", line 184, in train_dataloader\n    return DataLoader(self.train_dataset, batch_size=self.train_batch_size, shuffle=True,\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/lightning_fabric/utilities/data.py\", line 325, in wrapper\n    init(obj, *args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/lightning_fabric/utilities/data.py\", line 325, in wrapper\n    init(obj, *args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/lightning_fabric/utilities/data.py\", line 325, in wrapper\n    init(obj, *args, **kwargs)\n  [Previous line repeated 1 more time]\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 383, in __init__\n    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\", line 165, in __init__\n    raise ValueError(\nValueError: num_samples should be a positive integer value, but got num_samples=0\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Cell 6: Training Execution (Trying strategy=\"auto\")\n\n# --- Define the path to your pre-generated datasets on Kaggle ---\nPREPROCESSED_DATA_INPUT_DIR = \"/kaggle/input/nli-datasets-alignscore\" \n\n# --- Training Configuration ---\navailable_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n# num_gpus_to_use = 0\n# if available_gpus >= 2:\n#     print(f\"Found {available_gpus} GPUs. Attempting to use 2.\")\n#     num_gpus_to_use = 2 \n# elif available_gpus == 1:\n#     print(\"Found 1 GPU. Using 1 GPU.\")\n#     num_gpus_to_use = 1\n# else:\n#     print(\"No GPUs found. Using CPU.\")\nnum_gpus_to_use = 1 if torch.cuda.is_available() else 0\n\n# Let PyTorch Lightning choose the strategy automatically for multi-GPU\nauto_strategy_name = \"auto\" if num_gpus_to_use > 1 else None # Will be 'auto' or None\n\nTRAIN_ARGS = Namespace(\n    model_name_or_path=MODEL_NAME, \n    batch_size=8, # This will be per-device if PTL picks a DDP-like strategy with \"auto\"\n    num_epoch=3, \n    num_workers=min(4, os.cpu_count() // 2 if os.cpu_count() else 2),\n    learning_rate=1e-5, \n    adam_epsilon=1e-6, \n    weight_decay=0.01, \n    warmup_steps_portion=0.06, \n    ckpt_save_path=CHECKPOINT_SAVE_PATH, \n    ckpt_comment=\"kaggle_auto_strat_20k_alignscore_\", # Updated comment\n    devices=num_gpus_to_use, \n    accelerator='gpu' if num_gpus_to_use > 0 else 'cpu',\n    strategy_name_for_log = auto_strategy_name, # For logging/checkpoint naming\n    accumulate_grad_batches=4, \n    do_mlm=True, \n    use_pretrained_model=True,\n    mlm_loss_factor=0.5 \n)\n\nif num_gpus_to_use > 1 and TRAIN_ARGS.accelerator == 'gpu': # Check if GPU is actually used\n    print(f\"Effective batch size per optimization step (assuming DDP-like from auto): {TRAIN_ARGS.batch_size * num_gpus_to_use * TRAIN_ARGS.accumulate_grad_batches}\")\nelse:\n    print(f\"Effective batch size per optimization step: {TRAIN_ARGS.batch_size * TRAIN_ARGS.accumulate_grad_batches}\")\n\n\n# --- Prepare dataset config for DataLoader ---\ntrain_files_config_list = []\nval_files_config_list = []\ntry:\n    all_input_files = [f for f in os.listdir(PREPROCESSED_DATA_INPUT_DIR) if f.endswith(\"_20k.jsonl\")]\nexcept FileNotFoundError:\n    print(f\"ERROR: Could not find the directory {PREPROCESSED_DATA_INPUT_DIR}.\")\n    all_input_files = []\n\nif not all_input_files:\n    print(f\"No '_20k.jsonl' files found in {PREPROCESSED_DATA_INPUT_DIR}.\")\n    BEST_MODEL_PATH = None\nelse:\n    random.shuffle(all_input_files)\n    num_val_files = max(1, int(0.1 * len(all_input_files))) if len(all_input_files) > 1 else 0\n    if len(all_input_files) == 1 and num_val_files == 0 : # Ensure validation if only one file\n        num_val_files = 1 \n        \n    for i, f_name in enumerate(all_input_files):\n        dataset_name_key = f_name.replace(\"_20k.jsonl\", \"\")\n        task_type = DATASET_CONFIG.get(dataset_name_key, {}).get('task', 'unknown') \n        if task_type == 'unknown':\n             print(f\"Warning: Task type for {dataset_name_key} (from file {f_name}) not found in DATASET_CONFIG. Assigning 'unknown'.\")\n        file_info = {'path': os.path.join(PREPROCESSED_DATA_INPUT_DIR, f_name), 'task_type': task_type}\n        \n        # If only one file total, use it for both train and val\n        if len(all_input_files) == 1:\n            val_files_config_list.append(file_info)\n            train_files_config_list.append(file_info)\n            print(f\"Warning: Only one data file found ({f_name}). Using it for both training and validation.\")\n            break # Exit loop as we've handled the single file case\n        elif num_val_files > 0 and i < num_val_files : \n            val_files_config_list.append(file_info)\n        else:\n            train_files_config_list.append(file_info)\n            \n    if len(all_input_files) > 1: # This logic only applies if there was more than 1 file initially\n        if not train_files_config_list and val_files_config_list: \n            train_files_config_list = val_files_config_list \n            print(\"Warning: Using the same file(s) for training and validation as only one distinct set of files was available after split for val.\")\n        elif not val_files_config_list and train_files_config_list: \n            if train_files_config_list:\n                 val_files_config_list.append(train_files_config_list[0])\n                 print(f\"Warning: No dedicated validation files after split. Using {train_files_config_list[0]['path']} for validation.\")\n            else: \n                 print(\"Warning: No training files available to create a validation split from.\")\n\n\nif not train_files_config_list:\n    print(\"No training data files configured. Skipping training.\")\n    BEST_MODEL_PATH = None\nelse:\n    print(f\"Training with {len(train_files_config_list)} task files. Validation with {len(val_files_config_list)} task files.\")\n    print(\"Training files (sample):\", [item['path'] for item in train_files_config_list[:min(3, len(train_files_config_list))]], \"...\")\n    print(\"Validation files (sample):\", [item['path'] for item in val_files_config_list[:min(3, len(val_files_config_list))]], \"...\")\n\n    data_module = DSTDataLoader( \n        train_files_config_list=train_files_config_list,\n        val_files_config_list=val_files_config_list, \n        model_name=TRAIN_ARGS.model_name_or_path,\n        train_batch_size=TRAIN_ARGS.batch_size,\n        eval_batch_size=TRAIN_ARGS.batch_size * 2, \n        num_workers=TRAIN_ARGS.num_workers, \n        need_mlm=TRAIN_ARGS.do_mlm \n    )\n\n    align_model = BERTAlignModel( \n        model_name_or_path=TRAIN_ARGS.model_name_or_path,\n        using_pretrained=TRAIN_ARGS.use_pretrained_model,\n        adam_epsilon=TRAIN_ARGS.adam_epsilon,\n        learning_rate=TRAIN_ARGS.learning_rate,\n        weight_decay=TRAIN_ARGS.weight_decay,\n        warmup_steps_portion=TRAIN_ARGS.warmup_steps_portion,\n        mlm_loss_factor=TRAIN_ARGS.mlm_loss_factor\n    )\n    align_model.need_mlm = TRAIN_ARGS.do_mlm\n    \n    model_short_name = TRAIN_ARGS.model_name_or_path.split('/')[-1]\n    strategy_file_component = TRAIN_ARGS.strategy_name_for_log if TRAIN_ARGS.strategy_name_for_log else \"single_device\"\n    checkpoint_filename_template = (\n        f\"{TRAIN_ARGS.ckpt_comment}{model_short_name}_\"\n        f\"{strategy_file_component}_\" \n        f\"{'scratch_' if not TRAIN_ARGS.use_pretrained_model else ''}\"\n        f\"{'mlm_' if TRAIN_ARGS.do_mlm else 'no_mlm_'}\"\n        f\"all_{SAMPLES_PER_DATASET_TARGET // 1000}k_\"\n        f\"b{TRAIN_ARGS.batch_size}x{TRAIN_ARGS.devices if TRAIN_ARGS.devices > 0 else 1}x{TRAIN_ARGS.accumulate_grad_batches}\"\n    )\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n        dirpath=TRAIN_ARGS.ckpt_save_path,\n        filename=checkpoint_filename_template + \"_{epoch:02d}_{val_loss:.2f}\",\n        save_top_k=1, monitor=\"val_loss\", mode=\"min\", every_n_epochs=1, save_last=True \n    )\n    early_stop_callback = pl.callbacks.EarlyStopping(\n        monitor=\"val_loss\", min_delta=0.001, patience=3, verbose=True, mode=\"min\"\n    )\n    \n    trainer_kwargs = {\n        \"accelerator\": TRAIN_ARGS.accelerator,\n        \"devices\": TRAIN_ARGS.devices,\n        \"max_epochs\": TRAIN_ARGS.num_epoch,\n        \"callbacks\": [checkpoint_callback, early_stop_callback],\n        \"accumulate_grad_batches\": TRAIN_ARGS.accumulate_grad_batches,\n        \"precision\": \"16-mixed\" if TRAIN_ARGS.accelerator == 'gpu' else 32, \n        \"log_every_n_steps\": 20,\n    }\n\n    # Only set strategy if using multiple devices and strategy_name_for_log is 'auto'\n    if TRAIN_ARGS.devices > 1 and TRAIN_ARGS.strategy_name_for_log == \"auto\": \n        trainer_kwargs[\"strategy\"] = \"auto\" \n    elif TRAIN_ARGS.devices > 1 and TRAIN_ARGS.strategy_name_for_log : # If a specific strategy object was set\n         # This branch would be for when we successfully instantiate DataParallelStrategy object\n         # For now, we rely on \"auto\" string for multi-GPU as direct object instantiation failed\n         # If you revert to trying an explicit strategy object, you'd pass it here.\n         # trainer_kwargs[\"strategy\"] = multi_gpu_strategy_object # from previous attempts\n         print(f\"Warning: A specific strategy object was intended but 'auto' is being used or no multi-GPU strategy is set.\")\n         if \"strategy\" not in trainer_kwargs and TRAIN_ARGS.devices > 1: # Ensure auto is set if devices > 1 and no object\n             trainer_kwargs[\"strategy\"] = \"auto\"\n\n\n    \n    trainer = pl.Trainer(**trainer_kwargs)\n\n    print(f\"\\n--- Starting Training: {checkpoint_filename_template} ---\")\n    actual_strategy_used = trainer.strategy.__class__.__name__ if trainer.strategy else \"single_device\"\n    print(f\"Trainer config: Devices={TRAIN_ARGS.devices}, Strategy Actually Used by PTL: {actual_strategy_used}\")\n\n\n    try:\n        trainer.fit(align_model, datamodule=data_module)\n        \n        BEST_MODEL_PATH = checkpoint_callback.best_model_path\n        print(f\"Best checkpoint path from callback: {BEST_MODEL_PATH}\")\n        if not BEST_MODEL_PATH or not os.path.exists(BEST_MODEL_PATH):\n            print(\"Best model path not found from callback, trying 'last.ckpt'\")\n            last_ckpt_path = os.path.join(TRAIN_ARGS.ckpt_save_path, \"last.ckpt\") \n            if os.path.exists(last_ckpt_path):\n                 BEST_MODEL_PATH = last_ckpt_path\n            else: \n                 constructed_final_path = os.path.join(TRAIN_ARGS.ckpt_save_path, f\"{checkpoint_filename_template}_epoch={TRAIN_ARGS.num_epoch-1}.ckpt\") \n                 if os.path.exists(constructed_final_path):\n                     BEST_MODEL_PATH = constructed_final_path\n                 else: \n                     BEST_MODEL_PATH = os.path.join(TRAIN_ARGS.ckpt_save_path, f\"{checkpoint_filename_template}_final_manual_save.ckpt\")\n                     if not os.path.exists(BEST_MODEL_PATH): \n                          trainer.save_checkpoint(BEST_MODEL_PATH)\n\n        print(f\"Training finished. Best/Final model considered to be at: {BEST_MODEL_PATH if BEST_MODEL_PATH and os.path.exists(BEST_MODEL_PATH) else 'Not Found or Not Saved'}\")\n\n    except Exception as e:\n        print(f\"An error occurred during training: {e}\")\n        import traceback\n        traceback.print_exc()\n        BEST_MODEL_PATH = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T23:31:48.378415Z","iopub.execute_input":"2025-05-28T23:31:48.379133Z"}},"outputs":[{"name":"stdout","text":"Effective batch size per optimization step: 32\nTraining with 6 task files. Validation with 1 task files.\nTraining files (sample): ['/kaggle/input/nli-datasets-alignscore/doc_nli_20k.jsonl', '/kaggle/input/nli-datasets-alignscore/nli_fever_20k.jsonl', '/kaggle/input/nli-datasets-alignscore/anli_r3_20k.jsonl'] ...\nValidation files (sample): ['/kaggle/input/nli-datasets-alignscore/mnli_20k.jsonl'] ...\n\n--- Starting Training: kaggle_auto_strat_20k_alignscore_roberta-large_single_device_mlm_all_20k_b8x1x4 ---\nTrainer config: Devices=1, Strategy Actually Used by PTL: SingleDeviceStrategy\nSetting up training data for DataLoader...\nLoading data for DSTDataSet...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading individual JSONL files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"919f31727b6040fbb44de24f2ed4b9e5"}},"metadata":{}},{"name":"stdout","text":"DSTDataSet loaded a total of 116946 examples from 6 task files.\nSetting up validation data for DataLoader...\nLoading data for DSTDataSet...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading individual JSONL files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ee2be97a3524eadb49ab8ee63b713b5"}},"metadata":{}},{"name":"stdout","text":"DSTDataSet loaded a total of 20000 examples from 1 task files.\nScheduler: Using trainer.estimated_stepping_batches = 10965\nScheduler: num_warmup_steps = 657, num_training_steps = 10965\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77c6d34ae35c40068a08d07e02844b41"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# Inference and Evaluation Definitions","metadata":{}},{"cell_type":"code","source":"\n# --- Inferencer Class ---\nclass Inferencer:\n    def __init__(self, ckpt_path, model_name_or_path='roberta-large', batch_size=32, device='cuda', verbose=True):\n        self.device = device\n        self.model_name_or_path = model_name_or_path # Store for tokenizer\n        if ckpt_path and os.path.exists(ckpt_path):\n            print(f\"Loading model from checkpoint: {ckpt_path}\")\n            try:\n                # When loading, pass hparams explicitly if they are not saved or to override\n                # For AlignScore, the model_name_or_path is part of hparams for BERTAlignModel\n                self.model = BERTAlignModel.load_from_checkpoint(\n                    checkpoint_path=ckpt_path,\n                    model_name_or_path=model_name_or_path # Pass this to ensure config is loaded correctly\n                ).to(self.device)\n            except Exception as e:\n                 print(f\"Failed to load from checkpoint with explicit hparams, trying without: {e}\")\n                 self.model = BERTAlignModel.load_from_checkpoint(checkpoint_path=ckpt_path, strict=False).to(self.device)\n\n        elif using_pretrained_for_inference_only: # If no ckpt, load a pretrained base model for testing inference pipeline\n            print(f\"Warning: Checkpoint path {ckpt_path} not found. Loading PRETRAINED base model {model_name_or_path} for inference structure testing.\")\n            self.model = BERTAlignModel(model_name_or_path=model_name_or_path, using_pretrained=True).to(self.device)\n        else: # If neither checkpoint nor pretrained base model for inference\n             raise FileNotFoundError(f\"Checkpoint path {ckpt_path} not found and not configured to load a base pretrained model for inference.\")\n\n        self.model.eval()\n        self.batch_size = batch_size\n\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path, trust_remote_code=True)\n        try:\n            self.spacy_nlp = spacy.load('en_core_web_sm')\n        except Exception as e:\n            print(f\"Spacy model en_core_web_sm not loaded: {e}. Sentence tokenization for _sp modes might be affected.\")\n            self.spacy_nlp = None\n\n\n        self.softmax = nn.Softmax(dim=-1)\n        self.disable_progress_bar_in_inference = not verbose\n        self.nlg_eval_mode = 'nli_sp' \n        self.verbose = verbose\n        self.smart_type = 'smart-n' # For smart_doc method from original AlignScore, not used in primary nlg_eval\n\n    def chunks(self, lst, n):\n        for i in range(0, len(lst), n):\n             yield lst[i:i + n]\n\n    def batch_tokenize(self, premise: list, hypo: list):\n        assert isinstance(premise, list) and isinstance(hypo, list)\n        assert len(premise) == len(hypo), \"Premise and hypothesis should have the same length.\"\n        \n        batch_tokenized = []\n        for i in range(0, len(premise), self.batch_size):\n            batch_premise = premise[i:i+self.batch_size]\n            batch_hypo = hypo[i:i+self.batch_size]\n            try:\n                inputs = self.tokenizer(batch_premise, batch_hypo, truncation='only_first', \n                                        padding='max_length', max_length=self.tokenizer.model_max_length, \n                                        return_tensors='pt')\n            except Exception as e: # Fallback if only_first fails (e.g. both are too long)\n                # print(f\"Tokenizer warning (only_first): {e}. Truncating both sequences.\")\n                inputs = self.tokenizer(batch_premise, batch_hypo, truncation=True, \n                                        padding='max_length', max_length=self.tokenizer.model_max_length, \n                                        return_tensors='pt')\n            batch_tokenized.append(inputs)\n        return batch_tokenized\n\n    def inference(self, premise: list, hypo: list):\n        if isinstance(premise, str) and isinstance(hypo, str):\n            premise = [premise]\n            hypo = [hypo]\n        \n        tokenized_batches = self.batch_tokenize(premise, hypo)\n        \n        all_reg_logits = []\n        all_bin_probs = [] # Store probability of positive class (index 1)\n        all_tri_probs = [] # Store all three probabilities (entail, neutral, contradict)\n\n        for mini_batch_inputs in tqdm(tokenized_batches, desc=\"Inferencing Batch\", disable=self.disable_progress_bar_in_inference):\n            mini_batch_inputs = {k: v.to(self.device) for k, v in mini_batch_inputs.items()}\n            with torch.no_grad():\n                model_output_obj = self.model(mini_batch_inputs) # BERTAlignModel forward\n                \n                reg_logits = model_output_obj.reg_label_logits.cpu() # Shape: (batch, 1)\n                bin_logits = model_output_obj.seq_relationship_logits.cpu() # Shape: (batch, 2)\n                tri_logits = model_output_obj.tri_label_logits.cpu() # Shape: (batch, 3)\n\n                bin_probs = self.softmax(bin_logits)\n                tri_probs = self.softmax(tri_logits)\n            \n            all_reg_logits.append(reg_logits[:,0]) # Squeeze to (batch,)\n            all_bin_probs.append(bin_probs[:,1]) # Prob of positive class\n            all_tri_probs.append(tri_probs) \n        \n        final_reg = torch.cat(all_reg_logits) if all_reg_logits else torch.empty(0)\n        final_bin_pos_prob = torch.cat(all_bin_probs) if all_bin_probs else torch.empty(0)\n        final_tri_probs = torch.cat(all_tri_probs) if all_tri_probs else torch.empty(0)\n        \n        return final_reg, final_bin_pos_prob, final_tri_probs\n\n\n    def inference_per_example(self, premise: str, hypo: str):\n        if not self.spacy_nlp:\n            # Fallback to NLTK if spaCy isn't loaded\n            premise_sents = sent_tokenize(premise) if premise else ['']\n            hypo_sents = sent_tokenize(hypo) if hypo else ['']\n        else:\n            premise_sents = [s.text for s in self.spacy_nlp(premise).sents] if premise else ['']\n            hypo_sents = [s.text for s in self.spacy_nlp(hypo).sents] if hypo else ['']\n        \n        premise_sents = premise_sents or [''] # Ensure not empty list\n        hypo_sents = hypo_sents or ['']     # Ensure not empty list\n\n\n        # AlignScore's original logic for chunking long premise sentences\n        def chunk_premise_sents(sents, n_words_per_chunk=350): # Approx.\n            processed = []\n            for s in sents:\n                words = s.strip().split()\n                if len(words) > n_words_per_chunk * 1.2: # If a single sentence is too long\n                    for i in range(0, len(words), n_words_per_chunk):\n                        processed.append(' '.join(words[i:i+n_words_per_chunk]))\n                else:\n                    processed.append(s)\n            return processed if processed else ['']\n\n        processed_premise_sents = chunk_premise_sents(premise_sents)\n\n        premise_sent_mat = []\n        hypo_sents_mat = []\n        for p_sent in processed_premise_sents:\n            for h_sent in hypo_sents:\n                premise_sent_mat.append(p_sent)\n                hypo_sents_mat.append(h_sent)\n        \n        if not premise_sent_mat: # Handles cases where premise or hypo was empty\n            return 0.0\n\n        # Get all three types of scores/probabilities from the model\n        reg_scores, bin_pos_probs, tri_probs_all = self.inference(premise_sent_mat, hypo_sents_mat)\n\n        output_score_for_agg = None\n        if self.nlg_eval_mode == 'nli_sp':\n            # tri_probs_all has shape (num_pairs, 3). Index 0 for entailment (AlignScore convention)\n            output_score_for_agg = tri_probs_all[:, 0] if tri_probs_all.numel() > 0 else torch.empty(0)\n        elif self.nlg_eval_mode == 'bin_sp':\n            output_score_for_agg = bin_pos_probs if bin_pos_probs.numel() > 0 else torch.empty(0)\n        elif self.nlg_eval_mode == 'reg_sp':\n            output_score_for_agg = reg_scores if reg_scores.numel() > 0 else torch.empty(0)\n        else: \n            print(f\"Warning: Unknown nlg_eval_mode '{self.nlg_eval_mode}' in inference_per_example. Defaulting to NLI entailment prob.\")\n            output_score_for_agg = tri_probs_all[:, 0] if tri_probs_all.numel() > 0 else torch.empty(0)\n\n        if output_score_for_agg.numel() == 0: return 0.0\n\n        try:\n            output_score_matrix = output_score_for_agg.view(len(processed_premise_sents), len(hypo_sents))\n            # Aggregate: max score for each hypothesis sentence across all premise chunks, then mean\n            if output_score_matrix.numel() > 0:\n                max_scores_per_hypo_sent, _ = torch.max(output_score_matrix, dim=0) # Max along premise chunks\n                final_score = torch.mean(max_scores_per_hypo_sent).item()\n            else:\n                final_score = 0.0\n        except RuntimeError as e: # If view fails due to mismatched sizes (e.g. empty hypo_sents)\n            # print(f\"RuntimeError during score matrix view/aggregation: {e}. Num premise_sents: {len(processed_premise_sents)}, num hypo_sents: {len(hypo_sents)}, agg_score shape: {output_score_for_agg.shape}\")\n            final_score = output_score_for_agg.mean().item() if output_score_for_agg.numel() > 0 else 0.0\n\n        return final_score\n\n    def nlg_eval(self, premises: List[str], hypos: List[str]):\n        assert self.nlg_eval_mode is not None, \"Select NLG Eval mode for nlg_eval!\"\n\n        if (self.nlg_eval_mode == 'bin') or (self.nlg_eval_mode == 'nli') or (self.nlg_eval_mode == 'reg'):\n            reg_out, bin_out_pos_prob, tri_out_probs = self.inference(premises, hypos)\n            if self.nlg_eval_mode == 'nli':\n                # Return entailment probability (index 0)\n                return None, tri_out_probs[:, 0] if tri_out_probs.numel() > 0 else torch.empty(0), None\n            elif self.nlg_eval_mode == 'bin':\n                return None, bin_out_pos_prob, None\n            elif self.nlg_eval_mode == 'reg':\n                return None, reg_out, None\n        \n        elif (self.nlg_eval_mode == 'bin_sp') or (self.nlg_eval_mode == 'nli_sp') or (self.nlg_eval_mode == 'reg_sp'):\n            scores = [self.inference_per_example(p, h) for p, h in tqdm(zip(premises, hypos), total=len(premises), desc=f\"Eval {self.nlg_eval_mode}\", disable=not self.verbose)]\n            return None, torch.tensor(scores), None \n        \n        else:\n            raise ValueError(f\"Unrecognized NLG Eval mode: {self.nlg_eval_mode}\")\n\n\n# --- Evaluation Configuration (from evaluate.py) ---\n# These are critical for the Evaluator class to know how to process datasets\nHUGGINGFACE_DATASETS_EVAL = { # Renamed to avoid conflict if Cell 2's var is different\n    'stsb': ['glue', 'stsb', 'validation'],\n    'mrpc': ['glue', 'mrpc', 'test'], # AlignScore uses test for MRPC eval\n    'paws': ['paws', 'labeled_final', 'test'],\n    'mnli_matched': ['multi_nli', 'validation_matched'],\n    'mnli_mismatched': ['multi_nli', 'validation_mismatched'],\n    'nli_fever': ['pietrolesci/nli_fever', 'default', 'test'], # Original was 'dev', using 'test' if available\n    'doc_nli': ['saattrupdan/doc-nli', None, 'test'],\n    'sick': ['sick', 'default', 'test'], # For SICK-R (relatedness) and SICK-E (entailment)\n    'boolq': ['boolq', 'validation'],\n    'anli_r1': ['anli', 'plain_text', 'test_r1'], # ANLI rounds\n    'anli_r2': ['anli', 'plain_text', 'test_r2'],\n    'anli_r3': ['anli', 'plain_text', 'test_r3'],\n    'snli': ['snli', 'test'],\n    'vitaminc': ['tals/vitaminc', 'default', 'test'], # Ensure 'test' split exists for this config\n    'qqp': ['glue', 'qqp', 'validation'],\n    # Summarization/Factuality benchmarks often loaded from local JSON/CSV in original AlignScore\n    # Examples of how they might be structured if on HF:\n    # 'summeval': ['KETI-QueryLab/summeval', 'default', 'test'], # Fictional, replace with actual if exists\n    # 'qags_xsum': ['SOME_HF_PATH/qags_xsum', 'default', 'test'],\n}\n\n# ALL_TASKS maps dataset name to output type index (0:reg, 1:bin, 2:tri NLI)\nALL_TASKS_EVAL = { \n    'stsb': 0, 'sick': 0, # For sick-r, SICK-E would be 2\n    'race_m': 1,'race_h': 1, 'boolq': 1,\n    'anli_r1': 2, 'anli_r2': 2, 'anli_r3': 2, 'snli': 2,\n    'vitaminc': 2, # 0:SUPPORTS, 1:NEI, 2:REFUTES\n    'mrpc': 1, 'paws': 1, 'paws_qqp': 1, 'qqp': 1,\n    'mnli_matched': 2, 'mnli_mismatched': 2,\n    'nli_fever': 2, 'doc_nli': 1, # doc_nli often treated as binary entail vs not\n    # Summarization datasets typically use Pearson/Spearman with human scores (regression-like)\n    'summeval': 0, 'qags_xsum': 0, 'qags_cnndm': 0, 'frank': 0, 'xsumfaith': 0,\n    # Other GLUE/SuperGLUE that AlignScore evaluated\n    'rte': 2, 'cb': 2, 'axb':1, 'axg':2, # axg (WNLI GLUE diagnostic) uses NLI format\n    # Add more as per original evaluate.py if you have the data\n}\n\n\n# --- Simplified Evaluator Class Placeholder ---\n# Implementing the full Evaluator from AlignScore is extensive.\n# This is a basic structure to show how one might evaluate a single task.\nclass SimplifiedEvaluator:\n    def __init__(self, align_inferencer, target_samples=20000, verbose=True):\n        self.align_inferencer = align_inferencer\n        self.target_samples = target_samples\n        self.verbose = verbose\n\n    def evaluate_task(self, task_name, hf_id, hf_config, hf_split,\n                        text_a_field, text_b_field, label_field, metric_type, \n                        nlg_mode_for_task, label_map_func=None, is_multi_text_b=False):\n        if self.verbose: print(f\"\\n--- Evaluating on benchmark: {task_name} ---\")\n        try:\n            if hf_config:\n                dataset = load_dataset(hf_id, name=hf_config, split=hf_split, trust_remote_code=True)\n            else:\n                dataset = load_dataset(hf_id, split=hf_split, trust_remote_code=True)\n        except Exception as e:\n            if self.verbose: print(f\"Could not load dataset {task_name}: {e}\")\n            return None\n\n        # Sample the dataset\n        if len(dataset) > self.target_samples:\n            if self.verbose: print(f\"Sampling {self.target_samples} from {len(dataset)} for {task_name} evaluation.\")\n            indices = random.sample(range(len(dataset)), self.target_samples)\n            dataset = dataset.select(indices)\n        \n        premises = []\n        hypotheses = []\n        true_labels_orig = []\n\n        for example in tqdm(dataset, desc=f\"Preparing {task_name}\", disable=(not self.verbose)):\n            try:\n                premises.append(str(example[text_a_field]))\n                if is_multi_text_b: # e.g. for race where text_b is options\n                     # This needs specific handling based on how options are structured and combined with question\n                     # For simplicity, this generic evaluator won't handle complex multi-text_b scenarios deeply.\n                     # Assuming text_b_field is the primary one for hypotheses for now.\n                     hypotheses.append(str(example[text_b_field][0])) # Take first option as placeholder\n                else:\n                    hypotheses.append(str(example[text_b_field]))\n                true_labels_orig.append(example[label_field])\n            except KeyError as e:\n                # print(f\"KeyError {e} in example: {example}. Skipping.\")\n                continue\n            except Exception as e:\n                # print(f\"Error processing example: {e}. Example: {example}. Skipping.\")\n                continue\n        \n        if not premises or not hypotheses:\n            if self.verbose: print(f\"No data to evaluate for {task_name} after preparation.\")\n            return None\n\n        self.align_inferencer.nlg_eval_mode = nlg_mode_for_task\n        _, pred_scores, _ = self.align_inferencer.nlg_eval(premises, hypotheses)\n        pred_scores_list = pred_scores.tolist()\n\n        true_labels_processed = []\n        if label_map_func:\n            true_labels_processed = [label_map_func(l) for l in true_labels_orig]\n        else:\n            true_labels_processed = [float(l) if metric_type == 'regression' else int(l) for l in true_labels_orig]\n\n        results = {'task': task_name, 'num_samples': len(pred_scores_list)}\n        if not pred_scores_list or not true_labels_processed or len(pred_scores_list) != len(true_labels_processed):\n            if self.verbose: print(f\"Mismatch in pred ({len(pred_scores_list)}) and true ({len(true_labels_processed)}) counts for {task_name}.\")\n            return results\n\n\n        if metric_type == 'regression':\n            p_corr, _ = pearsonr(true_labels_processed, pred_scores_list)\n            s_corr, _ = spearmanr(true_labels_processed, pred_scores_list)\n            k_corr, _ = kendalltau(true_labels_processed, pred_scores_list)\n            results.update({'pearson': p_corr, 'spearman': s_corr, 'kendall': k_corr})\n            if self.verbose: print(f\"{task_name} Results: Pearson={p_corr:.4f}, Spearman={s_corr:.4f}\")\n        elif metric_type == 'binary_classification':\n            # Assuming pred_scores are probabilities for the positive class\n            pred_labels_binary = [1 if p > 0.5 else 0 for p in pred_scores_list]\n            acc = accuracy_score(true_labels_processed, pred_labels_binary)\n            f1 = f1_score(true_labels_processed, pred_labels_binary, average='binary' if len(set(true_labels_processed)) <=2 else 'weighted', zero_division=0)\n            try:\n                auc = roc_auc_score(true_labels_processed, pred_scores_list) # Use raw scores for AUC\n            except ValueError: # If only one class in true_labels_processed\n                auc = 0.0 \n            results.update({'accuracy': acc, 'f1': f1, 'auc': auc})\n            if self.verbose: print(f\"{task_name} Results: Accuracy={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\")\n        elif metric_type == 'multiclass_classification':\n            # Assuming pred_scores here are from an NLI head (e.g. entailment probability)\n            # For true multiclass, inferencer would need to return argmax or full logits.\n            # This simplified evaluator uses pred_scores directly for NLI as \"entailment score\"\n            # This is a placeholder. True multiclass (e.g. MNLI 3-way) needs different handling of pred_scores.\n            # If pred_scores are [prob_entail, prob_neutral, prob_contradict] from tri_out_probs in Inferencer:\n            # pred_idx = torch.argmax(pred_scores, dim=1).tolist()\n            # acc = accuracy_score(true_labels_processed, pred_idx)\n            # f1 = f1_score(true_labels_processed, pred_idx, average='weighted', zero_division=0)\n            # results.update({'accuracy': acc, 'f1_weighted': f1})\n            # For now, if it's NLI and we get entail_prob:\n            if self.verbose: print(f\"Multiclass for {task_name} needs specific handling of model's tri_label_logits. Current pred_scores are likely entail_probs.\")\n            # As a proxy for tasks like SNLI where entailment is label 0:\n            # Map true labels: 0->1 (entail), 1->0 (neutral), 2->0 (contradict)\n            # This is just an example for how one might binarize for an AUC like score if pred_scores are entail_probs\n            binary_true_for_auc = [1 if tl == 0 else 0 for tl in true_labels_processed] # Assuming 0 is entailment\n            try:\n                auc = roc_auc_score(binary_true_for_auc, pred_scores_list)\n                results.update({'auc_entail_vs_rest': auc})\n                if self.verbose: print(f\"{task_name} Results (AUC Entail vs Rest): AUC={auc:.4f}\")\n            except ValueError:\n                 results.update({'auc_entail_vs_rest': 0.0})\n\n\n        return results\n\n# You would also define baseline scorer classes (CTCScorer, SimCSEScorer, etc.) here\n# from `baselines.py` if you want to run them. This is omitted for brevity.\n# e.g. class SimCSEScorer: ...","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. Evaluation Preamble - Initialize Inferencer & Evaluator\n","metadata":{}},{"cell_type":"code","source":"\n# Ensure BEST_MODEL_PATH is set from training (Cell 6 output) or to a specific checkpoint path.\n# If BEST_MODEL_PATH is not set (e.g. training failed or was skipped), \n# set using_pretrained_for_inference_only = True and provide a MODEL_NAME\n# if you want to test the evaluation pipeline with a base Hugging Face model.\n\n# Example: If loading a downloaded AlignScore checkpoint:\n# DOWNLOADED_CKPT_PATH = \"/kaggle/working/pretrained_alignscore_checkpoints/AlignScore-large.ckpt\" # From download cell\n# EXPECTED_ROBERTA_VARIANT = \"roberta-large\" # Base model for the AlignScore checkpoint\n# BEST_MODEL_PATH = DOWNLOADED_CKPT_PATH \n# model_name_for_inferencer = EXPECTED_ROBERTA_VARIANT\n\n# If using model from training:\n# BEST_MODEL_PATH should be set by Cell 6\nmodel_name_for_inferencer = TRAIN_ARGS.model_name_or_path if 'TRAIN_ARGS' in globals() else MODEL_NAME\n\n\nalignscore_inferencer = None\n# This flag is for testing the eval pipeline with a base HF model if no checkpoint is available\nusing_pretrained_for_inference_only = False \n\nif BEST_MODEL_PATH and os.path.exists(BEST_MODEL_PATH):\n    print(f\"\\n--- Initializing Inferencer with trained model: {BEST_MODEL_PATH} ---\")\n    alignscore_inferencer = Inferencer(\n        ckpt_path=BEST_MODEL_PATH,\n        model_name_or_path=model_name_for_inferencer, \n        batch_size=TRAIN_ARGS.batch_size * 4 if 'TRAIN_ARGS' in globals() else 32, \n        device=DEVICE,\n        verbose=False \n    )\nelif using_pretrained_for_inference_only:\n    print(f\"\\n--- Initializing Inferencer with PRETRAINED base model: {model_name_for_inferencer} (NO FINETUNED CHECKPOINT) ---\")\n    alignscore_inferencer = Inferencer(\n        ckpt_path=None, \n        model_name_or_path=model_name_for_inferencer, \n        batch_size=TRAIN_ARGS.batch_size * 4 if 'TRAIN_ARGS' in globals() else 32,\n        device=DEVICE,\n        verbose=False\n    )\nelse:\n    print(\"No trained model checkpoint (BEST_MODEL_PATH) found or specified, and not using pretrained for inference. Cannot proceed with evaluation.\")\n\nevaluator = None\nif alignscore_inferencer:\n    evaluator = SimplifiedEvaluator(alignscore_inferencer, target_samples=1000, verbose=True)\n    print(\"Inferencer and SimplifiedEvaluator initialized.\")\nelse:\n    print(\"Evaluation components not initialized.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9: NLI FEVER Evaluation","metadata":{}},{"cell_type":"code","source":"\n\nall_eval_results_fever = []\n\nif evaluator: # Check if evaluator was initialized\n    # --- Evaluate NLI_FEVER ---\n    def nli_fever_label_map(label_val):\n        # pietrolesci/nli_fever labels: 0 (entail), 1 (neutral), 2 (contradict)\n        return int(label_val) \n\n    print(\"\\n--- Evaluating on NLI_FEVER ---\")\n    # Ensure 'nli_fever' is in HUGGINGFACE_DATASETS_EVAL and ALL_TASKS_EVAL (defined in Cell 7)\n    if 'nli_fever' in HUGGINGFACE_DATASETS_EVAL and 'nli_fever' in ALL_TASKS_EVAL:\n        hf_params = HUGGINGFACE_DATASETS_EVAL['nli_fever']\n        task_output_type_idx = ALL_TASKS_EVAL['nli_fever'] # Should be 2 for NLI\n\n        # Determine nlg_mode_for_task based on task_output_type_idx\n        # 0: reg_sp, 1: bin_sp, 2: nli_sp\n        nlg_mode = 'nli_sp' # Default for NLI type tasks\n        if task_output_type_idx == 0:\n            nlg_mode = 'reg_sp'\n        elif task_output_type_idx == 1:\n            nlg_mode = 'bin_sp'\n            \n        results_nli_fever = evaluator.evaluate_task(\n            task_name='nli_fever',\n            hf_id=hf_params[0], \n            hf_config=hf_params[1] if len(hf_params) > 2 else None, \n            hf_split=hf_params[-1], \n            text_a_field='premise',    # Corrected for pietrolesci/nli_fever\n            text_b_field='hypothesis', # Corrected for pietrolesci/nli_fever\n            label_field='label',\n            metric_type='multiclass_classification', \n            nlg_mode_for_task=nlg_mode, \n            label_map_func=nli_fever_label_map\n        )\n        if results_nli_fever: \n            all_eval_results_fever.append(results_nli_fever)\n            print(\"\\nNLI_FEVER Evaluation Results:\")\n            print(results_nli_fever)\n        else:\n            print(\"Evaluation failed or produced no results for nli_fever.\")\n    else:\n        print(\"nli_fever not found in HUGGINGFACE_DATASETS_EVAL or ALL_TASKS_EVAL configuration.\")\nelse:\n    print(\"Evaluator not initialized. Skipping NLI_FEVER evaluation.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10: Other NLI Datasets Evaluation\n","metadata":{}},{"cell_type":"code","source":"all_eval_results_other_nli = []\n\nif evaluator: # Check if evaluator was initialized\n    # Define the list of other NLI datasets to evaluate\n    # These names should match keys in HUGGINGFACE_DATASETS_EVAL and ALL_TASKS_EVAL (Cell 7)\n    # and correspond to your files in /kaggle/input/nli-datasets-alignscore/\n    other_nli_datasets_to_eval = {\n        \"mnli_matched\": {\n            \"text_a\": \"premise\", \"text_b\": \"hypothesis\", \"label\": \"label\", \n            \"metric\": \"multiclass_classification\", \"map_func\": lambda x: int(x)\n        },\n        \"snli\": {\n            \"text_a\": \"premise\", \"text_b\": \"hypothesis\", \"label\": \"label\",\n            \"metric\": \"multiclass_classification\", \"map_func\": lambda x: int(x) if x != -1 else 1 # Handle SNLI's -1 label\n        },\n        \"anli_r1\": {\n            \"text_a\": \"premise\", \"text_b\": \"hypothesis\", \"label\": \"label\",\n            \"metric\": \"multiclass_classification\", \"map_func\": lambda x: int(x)\n        },\n        \"anli_r2\": {\n            \"text_a\": \"premise\", \"text_b\": \"hypothesis\", \"label\": \"label\",\n            \"metric\": \"multiclass_classification\", \"map_func\": lambda x: int(x)\n        },\n        \"anli_r3\": {\n            \"text_a\": \"premise\", \"text_b\": \"hypothesis\", \"label\": \"label\",\n            \"metric\": \"multiclass_classification\", \"map_func\": lambda x: int(x)\n        },\n        \"doc_nli\": { # saattrupdan/doc-nli, labels are string 'entailment', 'neutral', 'contradiction'\n                     # ALL_TASKS_EVAL maps it to 1 (binary), so metric should be binary\n            \"text_a\": \"premise\", \"text_b\": \"hypothesis\", \"label\": \"label\",\n            \"metric\": \"binary_classification\", # As per ALL_TASKS_EVAL\n            \"map_func\": lambda x: 1 if str(x).lower() == 'entailment' else 0\n        }\n    }\n\n    for task_key, params in other_nli_datasets_to_eval.items():\n        print(f\"\\n--- Evaluating on {task_key} ---\")\n        if task_key in HUGGINGFACE_DATASETS_EVAL and task_key in ALL_TASKS_EVAL:\n            hf_params = HUGGINGFACE_DATASETS_EVAL[task_key]\n            task_output_type_idx = ALL_TASKS_EVAL[task_key]\n\n            nlg_mode = 'nli_sp' # Default for NLI\n            if task_output_type_idx == 1: # Binary task (like doc_nli)\n                nlg_mode = 'bin_sp'\n            \n            results = evaluator.evaluate_task(\n                task_name=task_key,\n                hf_id=hf_params[0],\n                hf_config=hf_params[1] if len(hf_params) > 2 else None,\n                hf_split=hf_params[-1],\n                text_a_field=params[\"text_a\"],\n                text_b_field=params[\"text_b\"],\n                label_field=params[\"label\"],\n                metric_type=params[\"metric\"],\n                nlg_mode_for_task=nlg_mode,\n                label_map_func=params[\"map_func\"]\n            )\n            if results:\n                all_eval_results_other_nli.append(results)\n                print(f\"\\n{task_key} Evaluation Results:\")\n                print(results)\n            else:\n                print(f\"Evaluation failed or produced no results for {task_key}.\")\n        else:\n            print(f\"{task_key} not found in HUGGINGFACE_DATASETS_EVAL or ALL_TASKS_EVAL configuration.\")\n    \n    print(\"\\n--- All Other NLI Evaluation Results ---\")\n    for res in all_eval_results_other_nli:\n        print(res)\nelse:\n    print(\"Evaluator not initialized. Skipping other NLI dataset evaluations.\")\n\n```\n\n**Before Running These Cells:**\n\n1.  **Cell 7 Definitions:** Ensure `Inferencer`, `SimplifiedEvaluator`, `HUGGINGFACE_DATASETS_EVAL`, and `ALL_TASKS_EVAL` are correctly defined in Cell 7.\n    * Make sure `HUGGINGFACE_DATASETS_EVAL` includes entries for `mnli_matched`, `snli`, `anli_r1`, `anli_r2`, `anli_r3`, and `doc_nli` with the correct Hugging Face IDs, configurations, and **test/validation splits**.\n    * Ensure `ALL_TASKS_EVAL` correctly maps these dataset keys to their output type index (mostly `2` for NLI, but `1` for `doc_nli` if you're treating it as binary entailment vs. not).\n2.  **`BEST_MODEL_PATH`:** This variable must be set correctly in Cell 8, either from the output of your training (Cell 6) or by manually pointing it to a downloaded AlignScore checkpoint.\n3.  **`TRAIN_ARGS` or `MODEL_NAME` for Inferencer:** The `Inferencer` needs the base model name (e.g., \"roberta-large\") to load the tokenizer and potentially the model config if loading from a checkpoint. Cell 8 tries to get this from `TRAIN_ARGS.model_name_or_path` or a global `MODEL_NAME`. Ensure one of these is correctly set.\n4.  **Run Order:**\n    * Run Cell 1 (Setup)\n    * Run Cell 2 (DataGenerator definitions)\n    * Run Cell 4 (DataLoader definitions)\n    * Run Cell 5 (BERTAlignModel definition)\n    * Run Cell 6 (Training, which sets `BEST_MODEL_PATH`) or manually set `BEST_MODEL_PATH` if using a pre-trained checkpoint.\n    * Run Cell 7 (Evaluation class definitions).\n    * Then run the new Cell 8 (Preamble).\n    * Then run Cell 9 (FEVER NLI Eval).\n    * Then run Cell 10 (Other NLI Datasets Eval).\n\nThis structure provides the separation you requested and should allow you to evaluate `nli_fever` and other NLI datasets distinctly. Remember that the quality of evaluation for `multiclass_classification` with the current `SimplifiedEvaluator` relies on `auc_entail_vs_rest`; for full 3-class accuracy/F1, the evaluator would need modificati","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# OLD eval cells","metadata":{}},{"cell_type":"code","source":"# # Ensure BEST_MODEL_PATH is set from training cell, or manually set to a checkpoint path\n# # BEST_MODEL_PATH = \"/kaggle/working/alignscore_checkpoints/your_best_model.ckpt\" # Example\n\n# # Initialize alignscore_inferencer to None\n# alignscore_inferencer = None\n# using_pretrained_for_inference_only = False # Set to True if BEST_MODEL_PATH is None and you want to test with a base model\n\n# if BEST_MODEL_PATH and os.path.exists(BEST_MODEL_PATH):\n#     print(f\"\\n--- Initializing Inferencer with trained model: {BEST_MODEL_PATH} ---\")\n#     alignscore_inferencer = Inferencer(\n#         ckpt_path=BEST_MODEL_PATH,\n#         model_name_or_path=TRAIN_ARGS.model_name_or_path, # From training args\n#         batch_size=TRAIN_ARGS.batch_size * 4, \n#         device=DEVICE,\n#         verbose=False # Less verbose during bulk eval\n#     )\n# elif using_pretrained_for_inference_only:\n#     print(f\"\\n--- Initializing Inferencer with PRETRAINED base model: {MODEL_NAME} (NO FINETUNED CHECKPOINT) ---\")\n#     alignscore_inferencer = Inferencer(\n#         ckpt_path=None, # No checkpoint\n#         model_name_or_path=MODEL_NAME, # Defined in Cell 1\n#         batch_size=TRAIN_ARGS.batch_size * 4, \n#         device=DEVICE,\n#         verbose=False\n#     )\n# else:\n#     print(\"No trained model checkpoint (BEST_MODEL_PATH) found or specified, and not using pretrained for inference. Skipping evaluation.\")\n#     # alignscore_inferencer remains None\n\n\n# if alignscore_inferencer: # Now this check is safe\n#     evaluator = SimplifiedEvaluator(alignscore_inferencer, target_samples=1000, verbose=True) \n\n#     all_eval_results = []\n\n#     # Example: Evaluate STS-B (Regression)\n#     def stsb_label_map(label): return float(label) / 5.0\n#     results_stsb = evaluator.evaluate_task(\n#         task_name='stsb',\n#         hf_id='glue', hf_config='stsb', hf_split='validation',\n#         text_a_field='sentence1', text_b_field='sentence2', label_field='label',\n#         metric_type='regression', nlg_mode_for_task='reg_sp',\n#         label_map_func=stsb_label_map\n#     )\n#     if results_stsb: all_eval_results.append(results_stsb)\n\n#     # Example: Evaluate MRPC (Binary Classification)\n#     results_mrpc = evaluator.evaluate_task(\n#         task_name='mrpc',\n#         hf_id='glue', hf_config='mrpc', hf_split='test', \n#         text_a_field='sentence1', text_b_field='sentence2', label_field='label',\n#         metric_type='binary_classification', nlg_mode_for_task='bin_sp'\n#     )\n#     if results_mrpc: all_eval_results.append(results_mrpc)\n\n#     # Example: Evaluate MNLI Matched (Multiclass NLI -> Simplified to Entailment Probability AUC)\n#     def nli_label_map(label): return int(label) \n#     results_mnli = evaluator.evaluate_task(\n#         task_name='mnli_matched',\n#         hf_id='multi_nli', hf_config=None, hf_split='validation_matched',\n#         text_a_field='premise', text_b_field='hypothesis', label_field='label',\n#         metric_type='multiclass_classification', nlg_mode_for_task='nli_sp', \n#         label_map_func=nli_label_map\n#     )\n#     if results_mnli: all_eval_results.append(results_mnli)\n    \n#     def boolq_label_map(label): return 1 if label else 0\n#     results_boolq = evaluator.evaluate_task(\n#         task_name='boolq',\n#         hf_id='boolq', hf_config=None, hf_split='validation',\n#         text_a_field='passage', text_b_field='question', label_field='answer', \n#         metric_type='binary_classification', nlg_mode_for_task='bin_sp', \n#         label_map_func=boolq_label_map\n#     )\n#     if results_boolq: all_eval_results.append(results_boolq)\n\n\n#     print(\"\\n--- All Evaluation Results ---\")\n#     for res in all_eval_results:\n#         print(res)\n# else:\n#     print(\"alignscore_inferencer was not initialized. Skipping evaluation execution.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T01:36:52.366097Z","iopub.execute_input":"2025-05-28T01:36:52.366407Z","iopub.status.idle":"2025-05-28T01:51:59.991683Z","shell.execute_reply.started":"2025-05-28T01:36:52.366376Z","shell.execute_reply":"2025-05-28T01:51:59.990818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Ensure BEST_MODEL_PATH is set from training cell, or manually set to a checkpoint path\n# # BEST_MODEL_PATH = \"/kaggle/working/alignscore_checkpoints/your_best_model.ckpt\" # Example if loading manually\n\n# using_pretrained_for_inference_only = False # Set to True if BEST_MODEL_PATH is None and you want to test with a base model\n\n# if BEST_MODEL_PATH and os.path.exists(BEST_MODEL_PATH):\n#     print(f\"\\n--- Initializing Inferencer with trained model: {BEST_MODEL_PATH} ---\")\n#     alignscore_inferencer = Inferencer(\n#         ckpt_path=BEST_MODEL_PATH,\n#         model_name_or_path=TRAIN_ARGS.model_name_or_path, # From training args\n#         batch_size=TRAIN_ARGS.batch_size * 4, \n#         device=DEVICE,\n#         verbose=False # Less verbose during bulk eval\n#     )\n# elif using_pretrained_for_inference_only:\n#     print(f\"\\n--- Initializing Inferencer with PRETRAINED base model: {MODEL_NAME} (NO FINETUNED CHECKPOINT) ---\")\n#     alignscore_inferencer = Inferencer(\n#         ckpt_path=None, # No checkpoint\n#         model_name_or_path=MODEL_NAME, # Defined in Cell 1\n#         batch_size=TRAIN_ARGS.batch_size * 4, \n#         device=DEVICE,\n#         verbose=False\n#     )\n# else:\n#     print(\"No trained model checkpoint (BEST_MODEL_PATH) found or specified, and not using pretrained for inference. Skipping evaluation.\")\n#     alignscore_inferencer = None\n\n\n# if alignscore_inferencer:\n#     evaluator = SimplifiedEvaluator(align_inferencer, target_samples=1000, verbose=True) # Use smaller sample for quick eval demo\n\n#     all_eval_results = []\n\n#     # Example: Evaluate STS-B (Regression)\n#     # Task: stsb, Output type index: 0 (regression) -> use 'reg_sp'\n#     # Labels are 0-5, need to be normalized to 0-1 if model output is in that range, or scale model output.\n#     # The original AlignScore normalizes STSB labels by dividing by 5.0.\n#     def stsb_label_map(label): return float(label) / 5.0\n#     results_stsb = evaluator.evaluate_task(\n#         task_name='stsb',\n#         hf_id='glue', hf_config='stsb', hf_split='validation',\n#         text_a_field='sentence1', text_b_field='sentence2', label_field='label',\n#         metric_type='regression', nlg_mode_for_task='reg_sp',\n#         label_map_func=stsb_label_map\n#     )\n#     if results_stsb: all_eval_results.append(results_stsb)\n\n#     # Example: Evaluate MRPC (Binary Classification)\n#     # Task: mrpc, Output type index: 1 (binary) -> use 'bin_sp'\n#     results_mrpc = evaluator.evaluate_task(\n#         task_name='mrpc',\n#         hf_id='glue', hf_config='mrpc', hf_split='test', # Original AlignScore uses 'test' for MRPC\n#         text_a_field='sentence1', text_b_field='sentence2', label_field='label',\n#         metric_type='binary_classification', nlg_mode_for_task='bin_sp'\n#     )\n#     if results_mrpc: all_eval_results.append(results_mrpc)\n\n#     # Example: Evaluate MNLI Matched (Multiclass NLI -> Simplified to Entailment Probability AUC)\n#     # Task: mnli_matched, Output type index: 2 (NLI) -> use 'nli_sp' (gets entailment prob)\n#     def nli_label_map(label): return int(label) # 0:entail, 1:neutral, 2:contradict\n#     results_mnli = evaluator.evaluate_task(\n#         task_name='mnli_matched',\n#         hf_id='multi_nli', hf_config=None, hf_split='validation_matched',\n#         text_a_field='premise', text_b_field='hypothesis', label_field='label',\n#         metric_type='multiclass_classification', nlg_mode_for_task='nli_sp', # 'nli_sp' gets entailment score by default\n#         label_map_func=nli_label_map\n#     )\n#     if results_mnli: all_eval_results.append(results_mnli)\n    \n#     # Example: Evaluate BoolQ (Binary Classification from QA format)\n#     # Task: boolq, Output type index: 1 (binary) -> use 'bin_sp'\n#     # Labels are True/False, map to 1/0\n#     def boolq_label_map(label): return 1 if label else 0\n#     # BoolQ is tricky with text_a/text_b for a generic evaluator.\n#     # Original AlignScore combines passage+question vs yes/no options.\n#     # This simplified one would take passage as premise, question as hypothesis.\n#     # For BoolQ, you might need a more specific `evaluate_boolq` method as in original.\n#     # Using a simplified setup here:\n#     results_boolq = evaluator.evaluate_task(\n#         task_name='boolq',\n#         hf_id='boolq', hf_config=None, hf_split='validation',\n#         text_a_field='passage', text_b_field='question', label_field='answer', # True/False\n#         metric_type='binary_classification', nlg_mode_for_task='bin_sp', # Model should predict if question is entailed by passage\n#         label_map_func=boolq_label_map\n#     )\n#     if results_boolq: all_eval_results.append(results_boolq)\n\n\n#     print(\"\\n--- All Evaluation Results ---\")\n#     for res in all_eval_results:\n#         print(res)\n        \n#     # To run baselines, you would instantiate their scorer classes and pass their .score/.scorer method\n#     # to the evaluator, similar to how `align_inferencer.nlg_eval` is used.\n#     # Example (pseudo-code, actual baseline classes need full definition from baselines.py):\n#     # if False: # Set to True to run example baseline\n#     #     simcse_scorer = SimCSEScorer(model_type='princeton-nlp/sup-simcse-roberta-large', device=DEVICE)\n#     #     evaluator_simcse = SimplifiedEvaluator(simcse_scorer, target_samples=1000) # SimCSE score method matches align_func\n#     #     results_stsb_simcse = evaluator_simcse.evaluate_task(...) # Call for STSB with SimCSE\n#     #     print(\"SimCSE STSB:\", results_stsb_simcse)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T01:34:32.971959Z","iopub.execute_input":"2025-05-28T01:34:32.972521Z","iopub.status.idle":"2025-05-28T01:35:04.870346Z","shell.execute_reply.started":"2025-05-28T01:34:32.972496Z","shell.execute_reply":"2025-05-28T01:35:04.869446Z"}},"outputs":[],"execution_count":null}]}