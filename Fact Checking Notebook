{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- Essential Installs ---\n!pip install -q transformers datasets pytorch-lightning nltk spacy scikit-learn torchmetrics pandas summa-extractive-summarizer sentence_transformers ctc-score bleurt bert-score bart-score unieval questeval rouge\n\n# --- Imports from AlignScore codebase (will be grouped from various files) ---\nimport os\nimport json\nimport random\nimport math\nimport re\nimport pickle\nimport logging\nimport time #\nfrom argparse import Namespace # To replace ArgumentParser\n\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader, Dataset, Sampler\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, precision_recall_fscore_support,\n    roc_auc_score, matthews_corrcoef, balanced_accuracy_score\n) #\nfrom sklearn.metrics.pairwise import cosine_similarity #\nfrom scipy.stats import pearsonr, kendalltau, spearmanr #\nfrom nltk.tokenize import sent_tokenize\nimport nltk\nimport spacy\n\nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer,\n    AutoModel,\n    AutoModelForSequenceClassification,\n    BertForPreTraining, BertModel, RobertaModel, AlbertModel, AlbertForMaskedLM, RobertaForMaskedLM,\n    T5Tokenizer, T5ForConditionalGeneration, #\n    BartTokenizer, BartForConditionalGeneration, #\n    AdamW, get_linear_schedule_with_warmup\n)\n\n# --- Basic Configuration ---\n# Paths will be relative to Kaggle's environment\nKAGGLE_WORKING_DIR = \"/kaggle/working/\"\nKAGGLE_INPUT_DIR = \"/kaggle/input/\" # Assume datasets might be loaded as Kaggle datasets\n\n# For Data Generation & Training\nSAMPLES_PER_DATASET_TARGET = 20000\nGENERATED_DATA_DIR = os.path.join(KAGGLE_WORKING_DIR, \"alignscore_training_data_20k\")\nCHECKPOINT_SAVE_PATH = os.path.join(KAGGLE_WORKING_DIR, \"alignscore_checkpoints\")\nMODEL_NAME = \"roberta-large\" # Or other models supported by AlignScore\n\nos.makedirs(GENERATED_DATA_DIR, exist_ok=True)\nos.makedirs(CHECKPOINT_SAVE_PATH, exist_ok=True)\n\n# Download necessary NLTK/spaCy models\nnltk.download('punkt', quiet=True)\nnltk.download('stopwords', quiet=True) #\ntry:\n    spacy.load('en_core_web_sm')\nexcept OSError:\n    spacy.cli.download('en_core_web_sm')\n\n# Suppress excessive logging from libraries if needed\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\nlogging.getLogger(\"datasets\").setLevel(logging.ERROR)\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.INFO)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- DATASET_HUGGINGFACE and DATASET_CONFIG ---\n# (Copy definitions from generate_training_data.py)\nDATASET_HUGGINGFACE = {\n    'cnndm': ['cnn_dailymail', '3.0.0', 'train'],\n    'mnli': ['multi_nli', 'default', 'train'],\n    # ... (all entries from the source)\n    'gap': ['gap', 'train'],\n} #\n\nDATASET_CONFIG = {\n    'cnndm': {'task': 'summarization', 'text_a': 'article', 'text_b': 'highlights', 'label': None, 'huggingface': True},\n    'mnli': {'task': 'nli', 'text_a': 'premise', 'text_b': 'hypothesis', 'label': 'label', 'huggingface': True},\n    # ... (all entries, ensuring paths in 'data_path' are updated if used for non-HF datasets)\n    'gap': {'task': 'coreference', 'huggingface': True}, #\n} #\n\n\n# --- Helper Classes for Data Augmentation ---\n# QA2D Class\n# QAnswering Class\n# MLMGeneratorWithPairedData Class\n# ExtractiveSummarizationGenerator Class (using summa.summarizer)\n# (Define these classes as in generate_training_data.py)\n\nclass QA2D:\n    def __init__(self, batch_size=32, device='cuda', verbose=True) -> None:\n        from transformers import BartTokenizer, BartForConditionalGeneration\n        self.tokenizer = BartTokenizer.from_pretrained(\"MarkS/bart-base-qa2d\")\n        self.model = BartForConditionalGeneration.from_pretrained(\"MarkS/bart-base-qa2d\").to(device)\n        self.batch_size = batch_size\n        self.device=device\n        self.verbose = verbose\n\n    def generate(self, questions: list, answers: list): #\n        assert len(questions) == len(answers) #\n        qa_list = []\n        for q, a in zip(questions, answers):\n            qa_list.append(f\"question: {q} answer: {a}\")\n        output = []\n        for qa_pairs in tqdm(\n            self.chunks(qa_list, self.batch_size),\n            desc=\"QA to Declarative\",\n            total=int(len(qa_list)/self.batch_size),\n            disable=(not self.verbose)\n        ): #\n            input_text = qa_pairs\n            input_token = self.tokenizer(\n                input_text, return_tensors='pt', padding=True, truncation=True).to(self.device)\n            dec_sents = self.model.generate(\n                 input_token.input_ids, max_length=512) #\n            result = self.tokenizer.batch_decode(\n                dec_sents, skip_special_tokens=True)\n            output.extend(result)\n        return output\n\n    def chunks(self, lst, n): #\n        \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n        for i in range(0, len(lst), n):\n             yield lst[i:i + n] #\n\n# ... (Other helper classes like QAnswering, MLMGeneratorWithPairedData, ExtractiveSummarizationGenerator)\n\n# --- DataGenerator Class ---\nclass DataGenerator: #\n    def __init__(self, dataset_names, target_samples_per_dataset=20000): #\n        self.dataset_names = dataset_names\n        self.datasets = dict()\n        self.t5_qa = None #\n        self.t5_tokenizer = None #\n        self.target_samples = target_samples_per_dataset\n        self.qa2d_generator = QA2D(device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False) # Initialize as needed\n        self.qa_generator = None # Initialize QAnswering if needed by uncommented QA tasks\n        self.mlm_hallucinator_cache = {} # For MLMGenerator\n\n        self.load_dataset_from_huggingface() #\n\n    def _get_mlm_hallucinator(self, corpus_name, corpra):\n        if corpus_name not in self.mlm_hallucinator_cache:\n            print(f\"Initializing MLM hallucinator for {corpus_name}...\")\n            self.mlm_hallucinator_cache[corpus_name] = MLMGeneratorWithPairedData(\n                corpra=corpra, device='cuda' if torch.cuda.is_available() else 'cpu', batch_size=64, mask_percent=0.25\n            )\n        return self.mlm_hallucinator_cache[corpus_name]\n\n    def load_dataset_from_huggingface(self): #\n        for each_dataset in self.dataset_names:\n            print(f\"Loading raw dataset: {each_dataset}\")\n            if DATASET_CONFIG[each_dataset].get('huggingface'):\n                try:\n                    self.datasets[each_dataset] = load_dataset(\n                         *DATASET_HUGGINGFACE[each_dataset][:-1])[DATASET_HUGGINGFACE[each_dataset][-1]] #\n                except Exception as e:\n                    print(f\"Could not load {each_dataset} from Hugging Face: {e}\")\n                    continue\n            # ... (add other loading mechanisms from original code if needed, e.g., pandas, json)\n            # Ensure paths for local files are correctly mapped in Kaggle if you add those.\n            else:\n                print(f\"Skipping {each_dataset} as it's not configured for Hugging Face loading in this simplified script.\")\n                continue\n            print(f\"Loaded {each_dataset} with {len(self.datasets.get(each_dataset, []))} examples.\")\n\n\n    # --- process_<dataset_name> methods ---\n    # (Each method will be adapted from generate_training_data.py)\n    # Example for one method:\n    def process_mnli(self): #\n        output = []\n        if 'mnli' not in self.datasets: return output\n        for example in tqdm(self.datasets['mnli'], desc=f'Constructing mnli', leave=False):\n            text_a = example[DATASET_CONFIG['mnli']['text_a']]\n            text_b = [example[DATASET_CONFIG['mnli']['text_b']]] #\n            text_c = [] #\n            label = example[DATASET_CONFIG['mnli']['label']] #\n            output.append({\n                'text_a': text_a,\n                'text_b': text_b,\n                'text_c': text_c,\n                'orig_label': label #\n            })\n        return output\n\n    def process_xsum(self): #\n        output = []\n        if 'xsum' not in self.datasets: return output\n        \n        original_data = self.datasets['xsum']\n        if len(original_data) > self.target_samples * 2: # Heuristic to speed up if source is very large before MLM\n            original_data = original_data.shuffle(seed=42).select(range(self.target_samples * 2))\n\n        gold_summaries = [ex[DATASET_CONFIG['xsum']['text_b']] for ex in original_data] #\n        \n        # For MLM, only process a subset if gold_summaries is huge to save time for demo\n        sample_for_mlm_size = min(len(gold_summaries), 5000) # Limit MLM for very large datasets in notebook\n        sampled_gold_for_mlm = random.sample(gold_summaries, sample_for_mlm_size)\n        \n        mlm_hallucinator_gold = self._get_mlm_hallucinator(\"xsum_gold\", sampled_gold_for_mlm)\n        gold_summary_hallucinated_sampled = mlm_hallucinator_gold.generate()\n\n        # Create a way to map back or use a placeholder if not all are processed by MLM\n        hallucinated_map_gold = {orig: hall for orig, hall in zip(sampled_gold_for_mlm, gold_summary_hallucinated_sampled)}\n\n        for example in tqdm(original_data, desc=\"Constructing xsum\", leave=False):\n            text_a = example[DATASET_CONFIG['xsum']['text_a']]\n            current_gold_summary = example[DATASET_CONFIG['xsum']['text_b']]\n            text_b = [current_gold_summary] # No extractive summary for simplicity here\n            \n            # Use hallucinated version if available, otherwise original as placeholder for text_c\n            text_c = [hallucinated_map_gold.get(current_gold_summary, current_gold_summary)] \n            \n            output.append({\n                'text_a': text_a,\n                'text_b': text_b,\n                'text_c': text_c, #\n                'orig_label': -1 #\n            })\n        return output\n        \n    # ... (Implement other process_<dataset_name> methods similarly,\n    #      focusing on using self.datasets[dataset_name] and returning a list of dicts.\n    #      For QA tasks needing QA2D or QAnswering, ensure those generators are initialized.\n    #      For summarization tasks needing MLM, they might be slow. Consider simplifying or running on smaller subsets.\n    # )\n\n    def generate(self): #\n        all_processed_data = {}\n        for each_dataset_name in self.dataset_names:\n            if each_dataset_name not in self.datasets or not self.datasets[each_dataset_name]:\n                print(f\"Dataset {each_dataset_name} not loaded or empty, skipping processing.\")\n                continue\n\n            print(f\"Processing {each_dataset_name}...\")\n            if hasattr(self, f'process_{each_dataset_name}'):\n                processed_examples = getattr(self, f'process_{each_dataset_name}')()\n                \n                if processed_examples:\n                    # Sample to target_samples\n                    if len(processed_examples) > self.target_samples:\n                        processed_examples = random.sample(processed_examples, self.target_samples)\n                    \n                    all_processed_data[each_dataset_name] = processed_examples\n                    \n                    # Save individual sampled dataset\n                    output_path = os.path.join(GENERATED_DATA_DIR, f\"{each_dataset_name}_20k.jsonl\")\n                    with open(output_path, 'w', encoding='utf8') as outfile: #\n                        for record in processed_examples:\n                            # Add task_type based on DATASET_CONFIG\n                            record_to_write = {\n                                'task': DATASET_CONFIG[each_dataset_name]['task'],\n                                'text_a': record['text_a'], #\n                                'text_b': record['text_b'], #\n                                'text_c': record.get('text_c', []), #\n                                'orig_label': record['orig_label'] #\n                            }\n                            json.dump(record_to_write, outfile, ensure_ascii=False) #\n                            outfile.write('\\n') #\n                    print(f\"Saved {len(processed_examples)} samples for {each_dataset_name} to {output_path}\")\n                else:\n                    print(f\"No examples processed for {each_dataset_name}.\")\n            else:\n                print(f\"No process method found for {each_dataset_name}\")\n        \n        return all_processed_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Specify datasets to generate (can be a subset of DATASET_CONFIG.keys() for faster testing)\n# For a full run, it would be list(DATASET_CONFIG.keys())\n# WARNING: Processing all datasets, especially with MLM/QA generation, can be VERY time-consuming.\n# Start with a small subset like ['mnli', 'paws'] for testing the pipeline.\ndatasets_to_generate = ['mnli', 'paws', 'xsum'] # Example subset\n# datasets_to_generate = [\n#                         'mnli', 'snli', 'anli_r1', 'anli_r2', 'anli_r3', # NLI\n#                         'doc_nli', # bin_nli\n#                         # 'nli_fever', 'vitaminc', # fact_checking (vitaminc might need tals/vitaminc to be available)\n#                         'paws', 'paws_qqp', 'qqp', # paraphrase ('paws_qqp' needs local file or HF source)\n#                         # 'wiki103', # paraphrase (needs local file)\n#                         # 'squad_v2', # qa (uses QA2D, QAnswering)\n#                         # 'race', # qa (uses QA2D)\n#                         # 'xsum', # summarization (uses MLM)\n#                         # 'cnndm', # summarization (uses MLM)\n#                         # 'wikihow', # summarization (uses MLM, requires local file setup in original)\n#                         # 'msmarco', # ir\n#                         'stsb', 'sick', # sts\n#                         # 'boolq', 'eraser_multi_rc', 'quail', 'sciq', 'strategy_qa', # QA\n#                         # 'gap' # coreference\n#                        ]\n\n\nprint(\"Initializing DataGenerator...\")\ndata_gen = DataGenerator(datasets_to_generate, target_samples_per_dataset=SAMPLES_PER_DATASET_TARGET)\n\nprint(\"Starting data generation and sampling...\")\ngenerated_data_info = data_gen.generate()\nprint(\"\\nData generation and sampling complete.\")\nfor name, data in generated_data_info.items():\n    print(f\"Dataset: {name}, Samples generated: {len(data)}\")\n\n# Verify saved files\nprint(\"\\nGenerated files:\")\nfor f_name in os.listdir(GENERATED_DATA_DIR):\n    if f_name.endswith(\".jsonl\"):\n        print(os.path.join(GENERATED_DATA_DIR, f_name))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}